# Distributed Processing

> ğŸ¯ **ç›®çš„**: Celeryã‚’ä½¿ç”¨ã—ãŸå¤§è¦æ¨¡åˆ†æ•£ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…
> 
> ğŸ“Š **å¯¾è±¡**: Map-Reduceãƒ‘ã‚¿ãƒ¼ãƒ³ã€ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã€ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–
> 
> âš¡ **ç‰¹å¾´**: ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªåˆ†æ•£å‡¦ç†ã€å‹•çš„è² è·åˆ†æ•£ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ç›£è¦–

## åˆ†æ•£å‡¦ç†ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### åŸºæœ¬ãƒ‡ãƒ¼ã‚¿æ§‹é€ 

```python
import hashlib
import json
from typing import List, Dict, Any, Tuple
from celery import group, chord, chain
from celery.result import GroupResult
import numpy as np
from dataclasses import dataclass
import time

@dataclass
class ProcessingPartition:
    """å‡¦ç†ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³"""
    partition_id: str
    data_range: Tuple[int, int]
    parameters: Dict[str, Any]
    assigned_worker: str = None
    estimated_processing_time: float = 0.0
    
    def __post_init__(self):
        """åˆæœŸåŒ–å¾Œã®å‡¦ç†"""
        if not self.partition_id:
            # ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³IDç”Ÿæˆ
            content = f"{self.data_range}_{hash(str(self.parameters))}"
            self.partition_id = hashlib.md5(content.encode()).hexdigest()[:8]

@dataclass
class ProcessingJob:
    """åˆ†æ•£å‡¦ç†ã‚¸ãƒ§ãƒ–"""
    job_id: str
    dataset_id: str
    processing_function: str
    parameters: Dict[str, Any]
    partitions: List[ProcessingPartition]
    status: str = 'pending'
    start_time: float = 0.0
    end_time: float = 0.0
    result: Any = None
    error: str = None
```

### åˆ†æ•£å‡¦ç†ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼

```python
class DistributedProcessor:
    """åˆ†æ•£å‡¦ç†ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, celery_app):
        self.celery_app = celery_app
        self.active_jobs = {}
        self.job_history = []
        self.performance_stats = {
            'total_jobs': 0,
            'successful_jobs': 0,
            'failed_jobs': 0,
            'total_processing_time': 0.0
        }
    
    def process_large_dataset(
        self, 
        dataset_id: str, 
        processing_function: str,
        parameters: Dict[str, Any],
        partition_size: int = 10000,
        max_workers: int = None
    ) -> str:
        """å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†æ•£å‡¦ç†"""
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã®å–å¾—
        dataset_size = self._get_dataset_size(dataset_id)
        print(f"Processing dataset {dataset_id} with {dataset_size} items")
        
        # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³è¨ˆç”»ã®ä½œæˆ
        partitions = self._create_partitions(dataset_id, dataset_size, partition_size)
        
        # ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã®æœ€é©åŒ–
        if max_workers is None:
            max_workers = min(len(partitions), self._get_optimal_worker_count())
        
        # åˆ†æ•£å‡¦ç†ã‚¸ãƒ§ãƒ–ã®ä½œæˆ
        job_id = f"distributed_{dataset_id}_{int(time.time())}"
        
        job = ProcessingJob(
            job_id=job_id,
            dataset_id=dataset_id,
            processing_function=processing_function,
            parameters=parameters,
            partitions=partitions
        )
        
        # Map-Reduce ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã®å‡¦ç†
        map_tasks = []
        for partition in partitions:
            task_signature = distributed_map_task.s(
                partition.partition_id,
                partition.data_range,
                processing_function,
                parameters
            )
            map_tasks.append(task_signature)
        
        # Chord ãƒ‘ã‚¿ãƒ¼ãƒ³: Map ãƒ•ã‚§ãƒ¼ã‚º + Reduce ãƒ•ã‚§ãƒ¼ã‚º
        celery_job = chord(map_tasks)(
            distributed_reduce_task.s(dataset_id, processing_function)
        )
        
        # ã‚¸ãƒ§ãƒ–ã®è¿½è·¡
        self.active_jobs[job_id] = {
            'job': celery_job,
            'job_info': job,
            'start_time': time.time(),
            'status': 'running'
        }
        
        self.performance_stats['total_jobs'] += 1
        
        print(f"Started distributed job {job_id} with {len(partitions)} partitions")
        return job_id
    
    def _get_dataset_size(self, dataset_id: str) -> int:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã®å–å¾—"""
        try:
            with get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM dataset_items WHERE dataset_id = %s", (dataset_id,))
                result = cursor.fetchone()
                return result[0] if result else 0
        except Exception as e:
            logger.error(f"Failed to get dataset size for {dataset_id}: {e}")
            return 0
    
    def _create_partitions(
        self, 
        dataset_id: str, 
        dataset_size: int, 
        partition_size: int
    ) -> List[ProcessingPartition]:
        """ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ä½œæˆ"""
        
        partitions = []
        for i in range(0, dataset_size, partition_size):
            end_index = min(i + partition_size, dataset_size)
            
            partition = ProcessingPartition(
                partition_id=f"{dataset_id}_partition_{i//partition_size}",
                data_range=(i, end_index),
                parameters={
                    'dataset_id': dataset_id,
                    'partition_index': i//partition_size
                },
                estimated_processing_time=self._estimate_processing_time(end_index - i)
            )
            partitions.append(partition)
        
        return partitions
    
    def _estimate_processing_time(self, item_count: int) -> float:
        """å‡¦ç†æ™‚é–“ã®æ¨å®š"""
        # éå»ã®å®Ÿè¡Œãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæ¨å®š
        base_time_per_item = 0.001  # 1ã‚¢ã‚¤ãƒ†ãƒ å½“ãŸã‚Š1ms
        overhead = 0.5  # ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰500ms
        return (item_count * base_time_per_item) + overhead
    
    def _get_optimal_worker_count(self) -> int:
        """æœ€é©ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã®å–å¾—"""
        # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã«åŸºã¥ãæ¨å®š
        try:
            import psutil
            cpu_count = psutil.cpu_count()
            memory_gb = psutil.virtual_memory().total / (1024**3)
            
            # CPUæ•°ã¨ãƒ¡ãƒ¢ãƒªã«åŸºã¥ãæ¨å®š
            optimal_workers = min(
                cpu_count * 2,  # CPUæ•°ã®2å€
                int(memory_gb / 0.5),  # 500MBã‚ãŸã‚Š1ãƒ¯ãƒ¼ã‚«ãƒ¼
                20  # æœ€å¤§20ãƒ¯ãƒ¼ã‚«ãƒ¼
            )
            
            return max(optimal_workers, 2)  # æœ€ä½2ãƒ¯ãƒ¼ã‚«ãƒ¼
        except ImportError:
            return 4  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    def get_job_status(self, job_id: str) -> Dict[str, Any]:
        """ã‚¸ãƒ§ãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®å–å¾—"""
        if job_id not in self.active_jobs:
            # å±¥æ­´ã‹ã‚‰æ¤œç´¢
            for historical_job in self.job_history:
                if historical_job['job_id'] == job_id:
                    return historical_job
            return {"status": "not_found"}
        
        job_info = self.active_jobs[job_id]
        celery_job = job_info['job']
        
        # ã‚¸ãƒ§ãƒ–ã®çŠ¶æ…‹ç¢ºèª
        status_info = {
            'job_id': job_id,
            'status': job_info['status'],
            'partitions': len(job_info['job_info'].partitions),
            'start_time': job_info['start_time'],
            'dataset_id': job_info['job_info'].dataset_id,
            'processing_function': job_info['job_info'].processing_function
        }
        
        if celery_job.ready():
            try:
                result = celery_job.get()
                job_info['status'] = 'completed'
                job_info['result'] = result
                job_info['end_time'] = time.time()
                
                status_info.update({
                    'status': 'completed',
                    'result': result,
                    'end_time': job_info['end_time'],
                    'duration': job_info['end_time'] - job_info['start_time']
                })
                
                self.performance_stats['successful_jobs'] += 1
                self.performance_stats['total_processing_time'] += status_info['duration']
                
                # å±¥æ­´ã«ç§»å‹•
                self.job_history.append(status_info.copy())
                del self.active_jobs[job_id]
                
            except Exception as e:
                job_info['status'] = 'failed'
                job_info['error'] = str(e)
                job_info['end_time'] = time.time()
                
                status_info.update({
                    'status': 'failed',
                    'error': str(e),
                    'end_time': job_info['end_time']
                })
                
                self.performance_stats['failed_jobs'] += 1
                
                # å±¥æ­´ã«ç§»å‹•
                self.job_history.append(status_info.copy())
                del self.active_jobs[job_id]
        else:
            # é€²è¡Œä¸­ã®è©³ç´°æƒ…å ±
            try:
                # Map ã‚¿ã‚¹ã‚¯ã®é€²æ—ç¢ºèª
                map_results = []
                completed_partitions = 0
                
                for partition in job_info['job_info'].partitions:
                    # å„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯ï¼ˆå®Ÿè£…ä¾å­˜ï¼‰
                    completed_partitions += 1  # ç°¡ç•¥åŒ–
                
                status_info.update({
                    'progress': {
                        'completed_partitions': completed_partitions,
                        'total_partitions': len(job_info['job_info'].partitions),
                        'percentage': int((completed_partitions / len(job_info['job_info'].partitions)) * 100)
                    }
                })
            except Exception as e:
                logger.warning(f"Failed to get detailed progress for {job_id}: {e}")
        
        return status_info
    
    def cancel_job(self, job_id: str) -> bool:
        """ã‚¸ãƒ§ãƒ–ã®ã‚­ãƒ£ãƒ³ã‚»ãƒ«"""
        if job_id not in self.active_jobs:
            return False
        
        try:
            celery_job = self.active_jobs[job_id]['job']
            celery_job.revoke(terminate=True)
            
            self.active_jobs[job_id]['status'] = 'cancelled'
            self.active_jobs[job_id]['end_time'] = time.time()
            
            print(f"Job {job_id} cancelled")
            return True
        except Exception as e:
            logger.error(f"Failed to cancel job {job_id}: {e}")
            return False
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã®å–å¾—"""
        stats = self.performance_stats.copy()
        
        if stats['successful_jobs'] > 0:
            stats['average_processing_time'] = stats['total_processing_time'] / stats['successful_jobs']
        else:
            stats['average_processing_time'] = 0.0
        
        stats['success_rate'] = (stats['successful_jobs'] / stats['total_jobs'] * 100) if stats['total_jobs'] > 0 else 0.0
        stats['active_jobs'] = len(self.active_jobs)
        stats['historical_jobs'] = len(self.job_history)
        
        return stats
```

## Map-Reduceå®Ÿè£…

### Map ãƒ•ã‚§ãƒ¼ã‚ºã‚¿ã‚¹ã‚¯

```python
# åˆ†æ•£å‡¦ç†ã‚¿ã‚¹ã‚¯
@app.task(bind=True, base=BaseTask, queue='cpu_intensive')
def distributed_map_task(
    self, 
    partition_id: str,
    data_range: Tuple[int, int],
    processing_function: str,
    parameters: Dict[str, Any]
):
    """åˆ†æ•£Mapã‚¿ã‚¹ã‚¯"""
    
    start_index, end_index = data_range
    dataset_id = parameters.get('dataset_id', partition_id.split('_')[0])
    
    self.update_state(
        state='PROGRESS',
        meta={
            'partition_id': partition_id,
            'progress': 0,
            'status': f'Starting processing for partition {partition_id}',
            'data_range': data_range
        }
    )
    
    try:
        # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT item_index, data FROM dataset_items 
                WHERE dataset_id = %s AND item_index BETWEEN %s AND %s
                ORDER BY item_index
            """, (dataset_id, start_index, end_index - 1))
            
            partition_data = []
            for item_index, data_json in cursor.fetchall():
                partition_data.append({
                    'index': item_index,
                    'data': json.loads(data_json)
                })
        
        if not partition_data:
            logger.warning(f"No data found for partition {partition_id}")
            return {
                'partition_id': partition_id,
                'processed_count': 0,
                'results': [],
                'processing_time': 0,
                'warning': 'No data found'
            }
        
        # å‡¦ç†é–¢æ•°ã®å®Ÿè¡Œ
        processing_func = get_processing_function(processing_function)
        
        results = []
        total_items = len(partition_data)
        start_time = time.time()
        
        for i, item in enumerate(partition_data):
            # ã‚¢ã‚¤ãƒ†ãƒ ã®å‡¦ç†
            try:
                processed_item = processing_func(item['data'], parameters)
                processed_item['original_index'] = item['index']
                results.append(processed_item)
            except Exception as item_error:
                logger.error(f"Failed to process item {item['index']} in partition {partition_id}: {item_error}")
                # ã‚¨ãƒ©ãƒ¼ã‚¢ã‚¤ãƒ†ãƒ ã‚‚è¨˜éŒ²
                results.append({
                    'original_index': item['index'],
                    'error': str(item_error),
                    'status': 'failed'
                })
            
            # é€²æ—æ›´æ–°
            if i % 100 == 0 or i == total_items - 1:
                progress = int((i + 1) / total_items * 100)
                elapsed_time = time.time() - start_time
                items_per_second = (i + 1) / elapsed_time if elapsed_time > 0 else 0
                
                self.update_state(
                    state='PROGRESS',
                    meta={
                        'partition_id': partition_id,
                        'progress': progress,
                        'processed': i + 1,
                        'total': total_items,
                        'items_per_second': round(items_per_second, 2),
                        'elapsed_time': round(elapsed_time, 2)
                    }
                )
        
        # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³çµæœã®ä¿å­˜
        processing_time = time.time() - start_time
        partition_result = {
            'partition_id': partition_id,
            'processed_count': len(results),
            'successful_count': len([r for r in results if r.get('status') != 'failed']),
            'failed_count': len([r for r in results if r.get('status') == 'failed']),
            'results': results,
            'processing_time': processing_time,
            'items_per_second': len(results) / processing_time if processing_time > 0 else 0,
            'data_range': data_range
        }
        
        print(f"Completed partition {partition_id}: {len(results)} items in {processing_time:.2f}s")
        return partition_result
        
    except Exception as exc:
        logger.error(f"Map task failed for partition {partition_id}: {exc}")
        self.update_state(
            state='FAILURE',
            meta={
                'partition_id': partition_id,
                'error': str(exc),
                'traceback': traceback.format_exc()
            }
        )
        raise exc

@app.task(bind=True, base=BaseTask)
def distributed_reduce_task(self, map_results: List[Dict[str, Any]], dataset_id: str, processing_function: str):
    """åˆ†æ•£Reduceã‚¿ã‚¹ã‚¯"""
    
    self.update_state(
        state='PROGRESS',
        meta={'status': 'Starting reduce phase', 'progress': 0}
    )
    
    try:
        # Mapçµæœã®æ¤œè¨¼ã¨å‰å‡¦ç†
        valid_results = []
        failed_partitions = []
        
        for partition_result in map_results:
            if 'error' in partition_result:
                failed_partitions.append(partition_result['partition_id'])
                continue
            valid_results.append(partition_result)
        
        if failed_partitions:
            logger.warning(f"Some partitions failed: {failed_partitions}")
        
        # Mapçµæœã®é›†ç´„
        total_processed = 0
        total_successful = 0
        total_failed = 0
        all_results = []
        processing_stats = {
            'partitions_processed': len(valid_results),
            'partitions_failed': len(failed_partitions),
            'total_processing_time': 0.0,
            'average_items_per_second': 0.0
        }
        
        for partition_result in valid_results:
            total_processed += partition_result['processed_count']
            total_successful += partition_result.get('successful_count', partition_result['processed_count'])
            total_failed += partition_result.get('failed_count', 0)
            all_results.extend(partition_result['results'])
            processing_stats['total_processing_time'] += partition_result['processing_time']
        
        # å¹³å‡å‡¦ç†é€Ÿåº¦ã®è¨ˆç®—
        if valid_results:
            avg_items_per_second = sum(r.get('items_per_second', 0) for r in valid_results) / len(valid_results)
            processing_stats['average_items_per_second'] = round(avg_items_per_second, 2)
        
        self.update_state(
            state='PROGRESS',
            meta={'status': 'Aggregating results', 'progress': 50}
        )
        
        # æœ€çµ‚çš„ãªé›†ç´„å‡¦ç†
        reduce_func = get_reduce_function(processing_function)
        final_result = reduce_func(all_results)
        
        self.update_state(
            state='PROGRESS',
            meta={'status': 'Saving final result', 'progress': 90}
        )
        
        # æœ€çµ‚çµæœã®ä¿å­˜
        result_summary = {
            'dataset_id': dataset_id,
            'processing_function': processing_function,
            'total_processed': total_processed,
            'total_successful': total_successful,
            'total_failed': total_failed,
            'success_rate': (total_successful / total_processed * 100) if total_processed > 0 else 0,
            'final_result': final_result,
            'partitions_processed': processing_stats['partitions_processed'],
            'partitions_failed': processing_stats['partitions_failed'],
            'processing_stats': processing_stats,
            'completed_at': time.time()
        }
        
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT INTO distributed_processing_results 
                (dataset_id, processing_function, result_data, total_processed, success_rate, completed_at) 
                VALUES (%s, %s, %s, %s, %s, %s)
            """, (
                dataset_id, 
                processing_function,
                json.dumps(result_summary),
                total_processed,
                result_summary['success_rate'],
                result_summary['completed_at']
            ))
            conn.commit()
        
        print(f"Distributed processing completed for dataset {dataset_id}: {total_processed} items processed")
        return result_summary
        
    except Exception as exc:
        logger.error(f"Reduce task failed for dataset {dataset_id}: {exc}")
        self.update_state(
            state='FAILURE',
            meta={
                'error': str(exc),
                'traceback': traceback.format_exc()
            }
        )
        raise exc
```

## å‡¦ç†é–¢æ•°ã¨ãƒ˜ãƒ«ãƒ‘ãƒ¼

### å‹•çš„å‡¦ç†é–¢æ•°ã‚·ã‚¹ãƒ†ãƒ 

```python
def get_processing_function(function_name: str):
    """å‡¦ç†é–¢æ•°ã®å–å¾—"""
    
    processing_functions = {
        'data_validation': validate_data_item,
        'data_transformation': transform_data_item,
        'data_analysis': analyze_data_item,
        'data_aggregation': aggregate_data_item,
        'data_enrichment': enrich_data_item,
        'data_classification': classify_data_item,
    }
    
    func = processing_functions.get(function_name)
    if func is None:
        raise ValueError(f"Unknown processing function: {function_name}")
    
    return func

def get_reduce_function(function_name: str):
    """Reduceé–¢æ•°ã®å–å¾—"""
    
    reduce_functions = {
        'data_validation': lambda results: {
            'valid_count': sum(1 for r in results if r.get('valid', False) and r.get('status') != 'failed'),
            'invalid_count': sum(1 for r in results if not r.get('valid', False)),
            'error_count': sum(1 for r in results if r.get('status') == 'failed'),
            'validation_summary': aggregate_validation_results(results)
        },
        'data_transformation': lambda results: {
            'transformed_items': [r for r in results if r.get('status') != 'failed'],
            'transformation_summary': summarize_transformations(results)
        },
        'data_analysis': lambda results: {
            'total_items': len([r for r in results if r.get('status') != 'failed']),
            'statistics': calculate_statistics(results),
            'analysis_summary': create_analysis_summary(results)
        },
        'data_aggregation': lambda results: aggregate_all_results(results),
        'data_enrichment': lambda results: {
            'enriched_items': [r for r in results if r.get('status') != 'failed'],
            'enrichment_stats': calculate_enrichment_stats(results)
        },
        'data_classification': lambda results: {
            'classification_summary': summarize_classifications(results),
            'class_distribution': calculate_class_distribution(results)
        }
    }
    
    func = reduce_functions.get(function_name)
    if func is None:
        return lambda results: {'results': results}  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    return func
```

### å…·ä½“çš„ãªå‡¦ç†é–¢æ•°å®Ÿè£…

```python
# å…·ä½“çš„ãªå‡¦ç†é–¢æ•°
def validate_data_item(item: Dict[str, Any], parameters: Dict[str, Any]) -> Dict[str, Any]:
    """ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¤ãƒ†ãƒ ã®æ¤œè¨¼"""
    
    validation_rules = parameters.get('validation_rules', {})
    
    result = {
        'item_id': item.get('id'),
        'valid': True,
        'errors': [],
        'warnings': []
    }
    
    # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ¤œè¨¼
    required_fields = validation_rules.get('required_fields', [])
    for field in required_fields:
        if field not in item or item[field] is None or item[field] == '':
            result['valid'] = False
            result['errors'].append(f'Missing required field: {field}')
    
    # ãƒ‡ãƒ¼ã‚¿å‹ã®æ¤œè¨¼
    type_validations = validation_rules.get('type_validations', {})
    for field, expected_type in type_validations.items():
        if field in item and item[field] is not None:
            if not isinstance(item[field], expected_type):
                result['valid'] = False
                result['errors'].append(f'Invalid type for field {field}: expected {expected_type.__name__}')
    
    # å€¤ã®ç¯„å›²æ¤œè¨¼
    range_validations = validation_rules.get('range_validations', {})
    for field, range_config in range_validations.items():
        if field in item and item[field] is not None:
            value = item[field]
            min_val = range_config.get('min')
            max_val = range_config.get('max')
            
            if min_val is not None and value < min_val:
                result['valid'] = False
                result['errors'].append(f'Field {field} value {value} is below minimum {min_val}')
            
            if max_val is not None and value > max_val:
                result['valid'] = False
                result['errors'].append(f'Field {field} value {value} is above maximum {max_val}')
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œè¨¼ï¼ˆæ­£è¦è¡¨ç¾ï¼‰
    pattern_validations = validation_rules.get('pattern_validations', {})
    for field, pattern in pattern_validations.items():
        if field in item and item[field] is not None:
            import re
            if not re.match(pattern, str(item[field])):
                result['valid'] = False
                result['errors'].append(f'Field {field} does not match pattern {pattern}')
    
    return result

def transform_data_item(item: Dict[str, Any], parameters: Dict[str, Any]) -> Dict[str, Any]:
    """ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¤ãƒ†ãƒ ã®å¤‰æ›"""
    
    transformations = parameters.get('transformations', {})
    
    transformed_item = item.copy()
    
    # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ­£è¦åŒ–
    if 'normalize_fields' in transformations:
        for field in transformations['normalize_fields']:
            if field in transformed_item and isinstance(transformed_item[field], str):
                transformed_item[field] = transformed_item[field].lower().strip()
    
    # è¨ˆç®—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®è¿½åŠ 
    if 'calculated_fields' in transformations:
        for calc_field, expression in transformations['calculated_fields'].items():
            try:
                # å®‰å…¨ãªè¨ˆç®—å¼ã®è©•ä¾¡
                safe_dict = {k: v for k, v in transformed_item.items() if isinstance(k, str)}
                transformed_item[calc_field] = eval(expression, {"__builtins__": {}}, safe_dict)
            except Exception as e:
                transformed_item[calc_field] = None
                logger.warning(f"Failed to calculate field {calc_field}: {e}")
    
    # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å¤‰æ›
    if 'field_mappings' in transformations:
        for old_field, new_field in transformations['field_mappings'].items():
            if old_field in transformed_item:
                transformed_item[new_field] = transformed_item[old_field]
                if old_field != new_field:
                    del transformed_item[old_field]
    
    # ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›
    if 'type_conversions' in transformations:
        for field, target_type in transformations['type_conversions'].items():
            if field in transformed_item and transformed_item[field] is not None:
                try:
                    if target_type == 'int':
                        transformed_item[field] = int(transformed_item[field])
                    elif target_type == 'float':
                        transformed_item[field] = float(transformed_item[field])
                    elif target_type == 'str':
                        transformed_item[field] = str(transformed_item[field])
                    elif target_type == 'bool':
                        transformed_item[field] = bool(transformed_item[field])
                except (ValueError, TypeError) as e:
                    logger.warning(f"Failed to convert {field} to {target_type}: {e}")
    
    return transformed_item

def analyze_data_item(item: Dict[str, Any], parameters: Dict[str, Any]) -> Dict[str, Any]:
    """ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¤ãƒ†ãƒ ã®åˆ†æ"""
    
    analysis_config = parameters.get('analysis_config', {})
    
    analysis_result = {
        'item_id': item.get('id'),
        'metrics': {},
        'categories': [],
        'anomalies': [],
        'insights': []
    }
    
    # æ•°å€¤ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—
    numeric_fields = analysis_config.get('numeric_fields', [])
    for field in numeric_fields:
        if field in item and isinstance(item[field], (int, float)):
            analysis_result['metrics'][field] = {
                'value': item[field],
                'normalized': item[field] / analysis_config.get('normalization_factors', {}).get(field, 1),
                'percentile': calculate_percentile(item[field], field, analysis_config)
            }
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
    categorization_rules = analysis_config.get('categorization_rules', {})
    for category, rules in categorization_rules.items():
        if evaluate_categorization_rules(item, rules):
            analysis_result['categories'].append(category)
    
    # ç•°å¸¸æ¤œçŸ¥
    anomaly_detection = analysis_config.get('anomaly_detection', {})
    if anomaly_detection.get('enabled', False):
        anomalies = detect_anomalies(item, anomaly_detection)
        analysis_result['anomalies'].extend(anomalies)
    
    return analysis_result

def aggregate_data_item(item: Dict[str, Any], parameters: Dict[str, Any]) -> Dict[str, Any]:
    """ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¤ãƒ†ãƒ ã®é›†ç´„"""
    
    aggregation_config = parameters.get('aggregation_config', {})
    
    # ã‚°ãƒ«ãƒ¼ãƒ—ã‚­ãƒ¼ã®ç”Ÿæˆ
    group_fields = aggregation_config.get('group_by', [])
    group_key = tuple(item.get(field, 'null') for field in group_fields)
    
    # é›†ç´„å€¤ã®è¨ˆç®—
    aggregate_fields = aggregation_config.get('aggregate_fields', {})
    aggregated_values = {}
    
    for field, aggregation_type in aggregate_fields.items():
        if field in item:
            value = item[field]
            
            if aggregation_type == 'sum':
                aggregated_values[f'{field}_sum'] = value
            elif aggregation_type == 'count':
                aggregated_values[f'{field}_count'] = 1
            elif aggregation_type == 'avg':
                aggregated_values[f'{field}_sum'] = value
                aggregated_values[f'{field}_count'] = 1
            elif aggregation_type == 'min':
                aggregated_values[f'{field}_min'] = value
            elif aggregation_type == 'max':
                aggregated_values[f'{field}_max'] = value
    
    return {
        'group_key': group_key,
        'aggregated_values': aggregated_values,
        'original_item': item
    }
```

## ä½¿ç”¨ä¾‹ã¨æœ€é©åŒ–

### åˆ†æ•£å‡¦ç†å®Ÿè¡Œä¾‹

```python
# åˆ†æ•£å‡¦ç†ã®å®Ÿè¡Œä¾‹
distributed_processor = DistributedProcessor(app)

def run_distributed_processing_example():
    """åˆ†æ•£å‡¦ç†ã®å®Ÿè¡Œä¾‹"""
    
    # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã®åˆ†æ•£å‡¦ç†
    validation_job = distributed_processor.process_large_dataset(
        dataset_id="dataset_001",
        processing_function="data_validation",
        parameters={
            "validation_rules": {
                "required_fields": ["id", "name", "email"],
                "type_validations": {
                    "id": int,
                    "name": str,
                    "email": str
                },
                "pattern_validations": {
                    "email": r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
                }
            }
        },
        partition_size=5000
    )
    
    # ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã®åˆ†æ•£å‡¦ç†
    transformation_job = distributed_processor.process_large_dataset(
        dataset_id="dataset_001",
        processing_function="data_transformation",
        parameters={
            "transformations": {
                "normalize_fields": ["name", "email"],
                "calculated_fields": {
                    "name_length": "len(name) if 'name' in locals() and name else 0"
                },
                "type_conversions": {
                    "age": "int",
                    "score": "float"
                }
            }
        },
        partition_size=5000
    )
    
    print(f"Started validation job: {validation_job}")
    print(f"Started transformation job: {transformation_job}")
    
    return validation_job, transformation_job

def monitor_distributed_jobs():
    """åˆ†æ•£ã‚¸ãƒ§ãƒ–ã®ç›£è¦–ä¾‹"""
    
    # å®Ÿè¡Œä¸­ã®ã‚¸ãƒ§ãƒ–ç›£è¦–
    while distributed_processor.active_jobs:
        for job_id in list(distributed_processor.active_jobs.keys()):
            status = distributed_processor.get_job_status(job_id)
            
            print(f"Job {job_id}: {status['status']}")
            
            if 'progress' in status:
                progress = status['progress']
                print(f"  Progress: {progress['percentage']}% ({progress['completed_partitions']}/{progress['total_partitions']})")
            
            if status['status'] in ['completed', 'failed']:
                if status['status'] == 'completed':
                    print(f"  Duration: {status['duration']:.2f}s")
                    print(f"  Result summary: {status['result'].get('total_processed', 'N/A')} items")
                else:
                    print(f"  Error: {status['error']}")
        
        time.sleep(5)
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã®è¡¨ç¤º
    stats = distributed_processor.get_performance_stats()
    print("\nPerformance Statistics:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®ãƒ’ãƒ³ãƒˆ

```python
class OptimizedDistributedProcessor(DistributedProcessor):
    """æœ€é©åŒ–ã•ã‚ŒãŸåˆ†æ•£å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, celery_app):
        super().__init__(celery_app)
        self.optimization_config = {
            'dynamic_partitioning': True,
            'adaptive_worker_count': True,
            'load_balancing': True,
            'caching_enabled': True
        }
    
    def _create_optimized_partitions(self, dataset_id: str, dataset_size: int, base_partition_size: int):
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ä½œæˆ"""
        
        if not self.optimization_config['dynamic_partitioning']:
            return super()._create_partitions(dataset_id, dataset_size, base_partition_size)
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã«åŸºã¥ãå‹•çš„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºèª¿æ•´
        optimal_partition_size = self._calculate_optimal_partition_size(dataset_size, base_partition_size)
        
        return super()._create_partitions(dataset_id, dataset_size, optimal_partition_size)
    
    def _calculate_optimal_partition_size(self, dataset_size: int, base_size: int) -> int:
        """æœ€é©ãªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºã®è¨ˆç®—"""
        
        try:
            import psutil
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã«åŸºã¥ãèª¿æ•´
            memory_percent = psutil.virtual_memory().percent
            if memory_percent > 80:
                return max(base_size // 2, 1000)  # ãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚ã¯å°ã•ã
            elif memory_percent < 50:
                return min(base_size * 2, 50000)  # ãƒ¡ãƒ¢ãƒªä½™è£•æ™‚ã¯å¤§ãã
            
        except ImportError:
            pass
        
        return base_size
```