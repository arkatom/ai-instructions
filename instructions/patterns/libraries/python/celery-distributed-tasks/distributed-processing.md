# Distributed Processing

> üéØ **ÁõÆÁöÑ**: Celery„Çí‰ΩøÁî®„Åó„ÅüÂ§ßË¶èÊ®°ÂàÜÊï£„Éá„Éº„ÇøÂá¶ÁêÜ„Ç∑„Çπ„ÉÜ„É†„ÅÆÂÆüË£Ö
> 
> üìä **ÂØæË±°**: Map-Reduce„Éë„Çø„Éº„É≥„ÄÅ„Éá„Éº„Çø„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„ÄÅ‰∏¶ÂàóÂá¶ÁêÜÊúÄÈÅ©Âåñ
> 
> ‚ö° **ÁâπÂæ¥**: „Çπ„Ç±„Éº„É©„Éñ„É´„Å™ÂàÜÊï£Âá¶ÁêÜ„ÄÅÂãïÁöÑË≤†Ëç∑ÂàÜÊï£„ÄÅ„É™„Ç¢„É´„Çø„Ç§„É†ÈÄ≤ÊçóÁõ£Ë¶ñ

## ÂàÜÊï£Âá¶ÁêÜ„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£

### Âü∫Êú¨„Éá„Éº„ÇøÊßãÈÄ†

```python
import hashlib
import json
from typing import List, Dict, Any, Tuple
from celery import group, chord, chain
from celery.result import GroupResult
import numpy as np
from dataclasses import dataclass
import time

@dataclass
class ProcessingPartition:
    """Âá¶ÁêÜ„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥"""
    partition_id: str
    data_range: Tuple[int, int]
    parameters: Dict[str, Any]
    assigned_worker: str = None
    estimated_processing_time: float = 0.0
    
    def __post_init__(self):
        """ÂàùÊúüÂåñÂæå„ÅÆÂá¶ÁêÜ"""
        if not self.partition_id:
            # „Éè„ÉÉ„Ç∑„É•„Éô„Éº„Çπ„ÅÆ„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥IDÁîüÊàê
            content = f"{self.data_range}_{hash(str(self.parameters))}"
            self.partition_id = hashlib.md5(content.encode()).hexdigest()[:8]

@dataclass
class ProcessingJob:
    """ÂàÜÊï£Âá¶ÁêÜ„Ç∏„Éß„Éñ"""
    job_id: str
    dataset_id: str
    processing_function: str
    parameters: Dict[str, Any]
    partitions: List[ProcessingPartition]
    status: str = 'pending'
    start_time: float = 0.0
    end_time: float = 0.0
    result: Any = None
    error: str = None
```

### ÂàÜÊï£Âá¶ÁêÜ„Éû„Éç„Éº„Ç∏„É£„Éº

```python
class DistributedProcessor:
    """ÂàÜÊï£Âá¶ÁêÜÁÆ°ÁêÜ„ÇØ„É©„Çπ"""
    
    def __init__(self, celery_app):
        self.celery_app = celery_app
        self.active_jobs = {}
        self.job_history = []
        self.performance_stats = {
            'total_jobs': 0,
            'successful_jobs': 0,
            'failed_jobs': 0,
            'total_processing_time': 0.0
        }
    
    def process_large_dataset(
        self, 
        dataset_id: str, 
        processing_function: str,
        parameters: Dict[str, Any],
        partition_size: int = 10000,
        max_workers: int = None
    ) -> str:
        """Â§ßÂÆπÈáè„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂàÜÊï£Âá¶ÁêÜ"""
        
        # „Éá„Éº„Çø„Çª„ÉÉ„Éà„Çµ„Ç§„Ç∫„ÅÆÂèñÂæó
        dataset_size = self._get_dataset_size(dataset_id)
        print(f"Processing dataset {dataset_id} with {dataset_size} items")
        
        # „Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥Ë®àÁîª„ÅÆ‰ΩúÊàê
        partitions = self._create_partitions(dataset_id, dataset_size, partition_size)
        
        # „ÉØ„Éº„Ç´„ÉºÊï∞„ÅÆÊúÄÈÅ©Âåñ
        if max_workers is None:
            max_workers = min(len(partitions), self._get_optimal_worker_count())
        
        # ÂàÜÊï£Âá¶ÁêÜ„Ç∏„Éß„Éñ„ÅÆ‰ΩúÊàê
        job_id = f"distributed_{dataset_id}_{int(time.time())}"
        
        job = ProcessingJob(
            job_id=job_id,
            dataset_id=dataset_id,
            processing_function=processing_function,
            parameters=parameters,
            partitions=partitions
        )
        
        # Map-Reduce „Éë„Çø„Éº„É≥„Åß„ÅÆÂá¶ÁêÜ
        map_tasks = []
        for partition in partitions:
            task_signature = distributed_map_task.s(
                partition.partition_id,
                partition.data_range,
                processing_function,
                parameters
            )
            map_tasks.append(task_signature)
        
        # Chord „Éë„Çø„Éº„É≥: Map „Éï„Çß„Éº„Ç∫ + Reduce „Éï„Çß„Éº„Ç∫
        celery_job = chord(map_tasks)(
            distributed_reduce_task.s(dataset_id, processing_function)
        )
        
        # „Ç∏„Éß„Éñ„ÅÆËøΩË∑°
        self.active_jobs[job_id] = {
            'job': celery_job,
            'job_info': job,
            'start_time': time.time(),
            'status': 'running'
        }
        
        self.performance_stats['total_jobs'] += 1
        
        print(f"Started distributed job {job_id} with {len(partitions)} partitions")
        return job_id
    
    def _get_dataset_size(self, dataset_id: str) -> int:
        """„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çµ„Ç§„Ç∫„ÅÆÂèñÂæó"""
        try:
            with get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM dataset_items WHERE dataset_id = %s", (dataset_id,))
                result = cursor.fetchone()
                return result[0] if result else 0
        except Exception as e:
            logger.error(f"Failed to get dataset size for {dataset_id}: {e}")
            return 0
    
    def _create_partitions(
        self, 
        dataset_id: str, 
        dataset_size: int, 
        partition_size: int
    ) -> List[ProcessingPartition]:
        """„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥‰ΩúÊàê"""
        
        partitions = []
        for i in range(0, dataset_size, partition_size):
            end_index = min(i + partition_size, dataset_size)
            
            partition = ProcessingPartition(
                partition_id=f"{dataset_id}_partition_{i//partition_size}",
                data_range=(i, end_index),
                parameters={
                    'dataset_id': dataset_id,
                    'partition_index': i//partition_size
                },
                estimated_processing_time=self._estimate_processing_time(end_index - i)
            )
            partitions.append(partition)
        
        return partitions
    
    def _estimate_processing_time(self, item_count: int) -> float:
        """Âá¶ÁêÜÊôÇÈñì„ÅÆÊé®ÂÆö"""
        # ÈÅéÂéª„ÅÆÂÆüË°å„Éá„Éº„Çø„Å´Âü∫„Å•„ÅèÊé®ÂÆö
        base_time_per_item = 0.001  # 1„Ç¢„Ç§„ÉÜ„É†ÂΩì„Åü„Çä1ms
        overhead = 0.5  # „Ç™„Éº„Éê„Éº„Éò„ÉÉ„Éâ500ms
        return (item_count * base_time_per_item) + overhead
    
    def _get_optimal_worker_count(self) -> int:
        """ÊúÄÈÅ©„ÉØ„Éº„Ç´„ÉºÊï∞„ÅÆÂèñÂæó"""
        # „Ç∑„Çπ„ÉÜ„É†„É™„ÇΩ„Éº„Çπ„Å´Âü∫„Å•„ÅèÊé®ÂÆö
        try:
            import psutil
            cpu_count = psutil.cpu_count()
            memory_gb = psutil.virtual_memory().total / (1024**3)
            
            # CPUÊï∞„Å®„É°„É¢„É™„Å´Âü∫„Å•„ÅèÊé®ÂÆö
            optimal_workers = min(
                cpu_count * 2,  # CPUÊï∞„ÅÆ2ÂÄç
                int(memory_gb / 0.5),  # 500MB„ÅÇ„Åü„Çä1„ÉØ„Éº„Ç´„Éº
                20  # ÊúÄÂ§ß20„ÉØ„Éº„Ç´„Éº
            )
            
            return max(optimal_workers, 2)  # ÊúÄ‰Ωé2„ÉØ„Éº„Ç´„Éº
        except ImportError:
            return 4  # „Éá„Éï„Ç©„É´„ÉàÂÄ§
    
    def get_job_status(self, job_id: str) -> Dict[str, Any]:
        """„Ç∏„Éß„Éñ„Çπ„ÉÜ„Éº„Çø„Çπ„ÅÆÂèñÂæó"""
        if job_id not in self.active_jobs:
            # Â±•Ê≠¥„Åã„ÇâÊ§úÁ¥¢
            for historical_job in self.job_history:
                if historical_job['job_id'] == job_id:
                    return historical_job
            return {"status": "not_found"}
        
        job_info = self.active_jobs[job_id]
        celery_job = job_info['job']
        
        # „Ç∏„Éß„Éñ„ÅÆÁä∂ÊÖãÁ¢∫Ë™ç
        status_info = {
            'job_id': job_id,
            'status': job_info['status'],
            'partitions': len(job_info['job_info'].partitions),
            'start_time': job_info['start_time'],
            'dataset_id': job_info['job_info'].dataset_id,
            'processing_function': job_info['job_info'].processing_function
        }
        
        if celery_job.ready():
            try:
                result = celery_job.get()
                job_info['status'] = 'completed'
                job_info['result'] = result
                job_info['end_time'] = time.time()
                
                status_info.update({
                    'status': 'completed',
                    'result': result,
                    'end_time': job_info['end_time'],
                    'duration': job_info['end_time'] - job_info['start_time']
                })
                
                self.performance_stats['successful_jobs'] += 1
                self.performance_stats['total_processing_time'] += status_info['duration']
                
                # Â±•Ê≠¥„Å´ÁßªÂãï
                self.job_history.append(status_info.copy())
                del self.active_jobs[job_id]
                
            except Exception as e:
                job_info['status'] = 'failed'
                job_info['error'] = str(e)
                job_info['end_time'] = time.time()
                
                status_info.update({
                    'status': 'failed',
                    'error': str(e),
                    'end_time': job_info['end_time']
                })
                
                self.performance_stats['failed_jobs'] += 1
                
                # Â±•Ê≠¥„Å´ÁßªÂãï
                self.job_history.append(status_info.copy())
                del self.active_jobs[job_id]
        else:
            # ÈÄ≤Ë°å‰∏≠„ÅÆË©≥Á¥∞ÊÉÖÂ†±
            try:
                # Map „Çø„Çπ„ÇØ„ÅÆÈÄ≤ÊçóÁ¢∫Ë™ç
                map_results = []
                completed_partitions = 0
                
                for partition in job_info['job_info'].partitions:
                    # ÂêÑ„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„ÅÆÁä∂ÊÖã„ÉÅ„Çß„ÉÉ„ÇØÔºàÂÆüË£Ö‰æùÂ≠òÔºâ
                    completed_partitions += 1  # Á∞°Áï•Âåñ
                
                status_info.update({
                    'progress': {
                        'completed_partitions': completed_partitions,
                        'total_partitions': len(job_info['job_info'].partitions),
                        'percentage': int((completed_partitions / len(job_info['job_info'].partitions)) * 100)
                    }
                })
            except Exception as e:
                logger.warning(f"Failed to get detailed progress for {job_id}: {e}")
        
        return status_info
    
    def cancel_job(self, job_id: str) -> bool:
        """„Ç∏„Éß„Éñ„ÅÆ„Ç≠„É£„É≥„Çª„É´"""
        if job_id not in self.active_jobs:
            return False
        
        try:
            celery_job = self.active_jobs[job_id]['job']
            celery_job.revoke(terminate=True)
            
            self.active_jobs[job_id]['status'] = 'cancelled'
            self.active_jobs[job_id]['end_time'] = time.time()
            
            print(f"Job {job_id} cancelled")
            return True
        except Exception as e:
            logger.error(f"Failed to cancel job {job_id}: {e}")
            return False
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÁµ±Ë®à„ÅÆÂèñÂæó"""
        stats = self.performance_stats.copy()
        
        if stats['successful_jobs'] > 0:
            stats['average_processing_time'] = stats['total_processing_time'] / stats['successful_jobs']
        else:
            stats['average_processing_time'] = 0.0
        
        stats['success_rate'] = (stats['successful_jobs'] / stats['total_jobs'] * 100) if stats['total_jobs'] > 0 else 0.0
        stats['active_jobs'] = len(self.active_jobs)
        stats['historical_jobs'] = len(self.job_history)
        
        return stats
```

## Map-ReduceÂÆüË£Ö

### Map „Éï„Çß„Éº„Ç∫„Çø„Çπ„ÇØ

```python
# ÂàÜÊï£Âá¶ÁêÜ„Çø„Çπ„ÇØ
@app.task(bind=True, base=BaseTask, queue='cpu_intensive')
def distributed_map_task(
    self, 
    partition_id: str,
    data_range: Tuple[int, int],
    processing_function: str,
    parameters: Dict[str, Any]
):
    """ÂàÜÊï£Map„Çø„Çπ„ÇØ"""
    
    start_index, end_index = data_range
    dataset_id = parameters.get('dataset_id', partition_id.split('_')[0])
    
    self.update_state(
        state='PROGRESS',
        meta={
            'partition_id': partition_id,
            'progress': 0,
            'status': f'Starting processing for partition {partition_id}',
            'data_range': data_range
        }
    )
    
    try:
        # „Éá„Éº„Çø„ÅÆË™≠„ÅøËæº„Åø
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT item_index, data FROM dataset_items 
                WHERE dataset_id = %s AND item_index BETWEEN %s AND %s
                ORDER BY item_index
            """, (dataset_id, start_index, end_index - 1))
            
            partition_data = []
            for item_index, data_json in cursor.fetchall():
                partition_data.append({
                    'index': item_index,
                    'data': json.loads(data_json)
                })
        
        if not partition_data:
            logger.warning(f"No data found for partition {partition_id}")
            return {
                'partition_id': partition_id,
                'processed_count': 0,
                'results': [],
                'processing_time': 0,
                'warning': 'No data found'
            }
        
        # Âá¶ÁêÜÈñ¢Êï∞„ÅÆÂÆüË°å
        processing_func = get_processing_function(processing_function)
        
        results = []
        total_items = len(partition_data)
        start_time = time.time()
        
        for i, item in enumerate(partition_data):
            # „Ç¢„Ç§„ÉÜ„É†„ÅÆÂá¶ÁêÜ
            try:
                processed_item = processing_func(item['data'], parameters)
                processed_item['original_index'] = item['index']
                results.append(processed_item)
            except Exception as item_error:
                logger.error(f"Failed to process item {item['index']} in partition {partition_id}: {item_error}")
                # „Ç®„É©„Éº„Ç¢„Ç§„ÉÜ„É†„ÇÇË®òÈå≤
                results.append({
                    'original_index': item['index'],
                    'error': str(item_error),
                    'status': 'failed'
                })
            
            # ÈÄ≤ÊçóÊõ¥Êñ∞
            if i % 100 == 0 or i == total_items - 1:
                progress = int((i + 1) / total_items * 100)
                elapsed_time = time.time() - start_time
                items_per_second = (i + 1) / elapsed_time if elapsed_time > 0 else 0
                
                self.update_state(
                    state='PROGRESS',
                    meta={
                        'partition_id': partition_id,
                        'progress': progress,
                        'processed': i + 1,
                        'total': total_items,
                        'items_per_second': round(items_per_second, 2),
                        'elapsed_time': round(elapsed_time, 2)
                    }
                )
        
        # „Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥ÁµêÊûú„ÅÆ‰øùÂ≠ò
        processing_time = time.time() - start_time
        partition_result = {
            'partition_id': partition_id,
            'processed_count': len(results),
            'successful_count': len([r for r in results if r.get('status') != 'failed']),
            'failed_count': len([r for r in results if r.get('status') == 'failed']),
            'results': results,
            'processing_time': processing_time,
            'items_per_second': len(results) / processing_time if processing_time > 0 else 0,
            'data_range': data_range
        }
        
        print(f"Completed partition {partition_id}: {len(results)} items in {processing_time:.2f}s")
        return partition_result
        
    except Exception as exc:
        logger.error(f"Map task failed for partition {partition_id}: {exc}")
        self.update_state(
            state='FAILURE',
            meta={
                'partition_id': partition_id,
                'error': str(exc),
                'traceback': traceback.format_exc()
            }
        )
        raise exc

@app.task(bind=True, base=BaseTask)
def distributed_reduce_task(self, map_results: List[Dict[str, Any]], dataset_id: str, processing_function: str):
    """ÂàÜÊï£Reduce„Çø„Çπ„ÇØ"""
    
    self.update_state(
        state='PROGRESS',
        meta={'status': 'Starting reduce phase', 'progress': 0}
    )
    
    try:
        # MapÁµêÊûú„ÅÆÊ§úË®º„Å®ÂâçÂá¶ÁêÜ
        valid_results = []
        failed_partitions = []
        
        for partition_result in map_results:
            if 'error' in partition_result:
                failed_partitions.append(partition_result['partition_id'])
                continue
            valid_results.append(partition_result)
        
        if failed_partitions:
            logger.warning(f"Some partitions failed: {failed_partitions}")
        
        # MapÁµêÊûú„ÅÆÈõÜÁ¥Ñ
        total_processed = 0
        total_successful = 0
        total_failed = 0
        all_results = []
        processing_stats = {
            'partitions_processed': len(valid_results),
            'partitions_failed': len(failed_partitions),
            'total_processing_time': 0.0,
            'average_items_per_second': 0.0
        }
        
        for partition_result in valid_results:
            total_processed += partition_result['processed_count']
            total_successful += partition_result.get('successful_count', partition_result['processed_count'])
            total_failed += partition_result.get('failed_count', 0)
            all_results.extend(partition_result['results'])
            processing_stats['total_processing_time'] += partition_result['processing_time']
        
        # Âπ≥ÂùáÂá¶ÁêÜÈÄüÂ∫¶„ÅÆË®àÁÆó
        if valid_results:
            avg_items_per_second = sum(r.get('items_per_second', 0) for r in valid_results) / len(valid_results)
            processing_stats['average_items_per_second'] = round(avg_items_per_second, 2)
        
        self.update_state(
            state='PROGRESS',
            meta={'status': 'Aggregating results', 'progress': 50}
        )
        
        # ÊúÄÁµÇÁöÑ„Å™ÈõÜÁ¥ÑÂá¶ÁêÜ
        reduce_func = get_reduce_function(processing_function)
        final_result = reduce_func(all_results)
        
        self.update_state(
            state='PROGRESS',
            meta={'status': 'Saving final result', 'progress': 90}
        )
        
        # ÊúÄÁµÇÁµêÊûú„ÅÆ‰øùÂ≠ò
        result_summary = {
            'dataset_id': dataset_id,
            'processing_function': processing_function,
            'total_processed': total_processed,
            'total_successful': total_successful,
            'total_failed': total_failed,
            'success_rate': (total_successful / total_processed * 100) if total_processed > 0 else 0,
            'final_result': final_result,
            'partitions_processed': processing_stats['partitions_processed'],
            'partitions_failed': processing_stats['partitions_failed'],
            'processing_stats': processing_stats,
            'completed_at': time.time()
        }
        
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT INTO distributed_processing_results 
                (dataset_id, processing_function, result_data, total_processed, success_rate, completed_at) 
                VALUES (%s, %s, %s, %s, %s, %s)
            """, (
                dataset_id, 
                processing_function,
                json.dumps(result_summary),
                total_processed,
                result_summary['success_rate'],
                result_summary['completed_at']
            ))
            conn.commit()
        
        print(f"Distributed processing completed for dataset {dataset_id}: {total_processed} items processed")
        return result_summary
        
    except Exception as exc:
        logger.error(f"Reduce task failed for dataset {dataset_id}: {exc}")
        self.update_state(
            state='FAILURE',
            meta={
                'error': str(exc),
                'traceback': traceback.format_exc()
            }
        )
        raise exc
```

## Âá¶ÁêÜÈñ¢Êï∞„Å®„Éò„É´„Éë„Éº

### ÂãïÁöÑÂá¶ÁêÜÈñ¢Êï∞„Ç∑„Çπ„ÉÜ„É†

```python
def get_processing_function(function_name: str):
    """Âá¶ÁêÜÈñ¢Êï∞„ÅÆÂèñÂæó"""
    
    processing_functions = {
        'data_validation': validate_data_item,
        'data_transformation': transform_data_item,
        'data_analysis': analyze_data_item,
        'data_aggregation': aggregate_data_item,
        'data_enrichment': enrich_data_item,
        'data_classification': classify_data_item,
    }
    
    func = processing_functions.get(function_name)
    if func is None:
        raise ValueError(f"Unknown processing function: {function_name}")
    
    return func

def get_reduce_function(function_name: str):
    """ReduceÈñ¢Êï∞„ÅÆÂèñÂæó"""
    
    reduce_functions = {
        'data_validation': lambda results: {
            'valid_count': sum(1 for r in results if r.get('valid', False) and r.get('status') != 'failed'),
            'invalid_count': sum(1 for r in results if not r.get('valid', False)),
            'error_count': sum(1 for r in results if r.get('status') == 'failed'),
            'validation_summary': aggregate_validation_results(results)
        },
        'data_transformation': lambda results: {
            'transformed_items': [r for r in results if r.get('status') != 'failed'],
            'transformation_summary': summarize_transformations(results)
        },
        'data_analysis': lambda results: {
            'total_items': len([r for r in results if r.get('status') != 'failed']),
            'statistics': calculate_statistics(results),
            'analysis_summary': create_analysis_summary(results)
        },
        'data_aggregation': lambda results: aggregate_all_results(results),
        'data_enrichment': lambda results: {
            'enriched_items': [r for r in results if r.get('status') != 'failed'],
            'enrichment_stats': calculate_enrichment_stats(results)
        },
        'data_classification': lambda results: {
            'classification_summary': summarize_classifications(results),
            'class_distribution': calculate_class_distribution(results)
        }
    }
    
    func = reduce_functions.get(function_name)
    if func is None:
        return lambda results: {'results': results}  # „Éá„Éï„Ç©„É´„Éà
    
    return func
```

### ÂÖ∑‰ΩìÁöÑ„Å™Âá¶ÁêÜÈñ¢Êï∞ÂÆüË£Ö

```python
# ÂÖ∑‰ΩìÁöÑ„Å™Âá¶ÁêÜÈñ¢Êï∞
def validate_data_item(item: Dict[str, Any], parameters: Dict[str, Any]) -> Dict[str, Any]:
    """„Éá„Éº„Çø„Ç¢„Ç§„ÉÜ„É†„ÅÆÊ§úË®º"""
    
    validation_rules = parameters.get('validation_rules', {})
    
    result = {
        'item_id': item.get('id'),
        'valid': True,
        'errors': [],
        'warnings': []
    }
    
    # ÂøÖÈ†à„Éï„Ç£„Éº„É´„Éâ„ÅÆÊ§úË®º
    required_fields = validation_rules.get('required_fields', [])
    for field in required_fields:
        if field not in item or item[field] is None or item[field] == '':
            result['valid'] = False
            result['errors'].append(f'Missing required field: {field}')
    
    # „Éá„Éº„ÇøÂûã„ÅÆÊ§úË®º
    type_validations = validation_rules.get('type_validations', {})
    for field, expected_type in type_validations.items():
        if field in item and item[field] is not None:
            if not isinstance(item[field], expected_type):
                result['valid'] = False
                result['errors'].append(f'Invalid type for field {field}: expected {expected_type.__name__}')
    
    # ÂÄ§„ÅÆÁØÑÂõ≤Ê§úË®º
    range_validations = validation_rules.get('range_validations', {})
    for field, range_config in range_validations.items():
        if field in item and item[field] is not None:
            value = item[field]
            min_val = range_config.get('min')
            max_val = range_config.get('max')
            
            if min_val is not None and value < min_val:
                result['valid'] = False
                result['errors'].append(f'Field {field} value {value} is below minimum {min_val}')
            
            if max_val is not None and value > max_val:
                result['valid'] = False
                result['errors'].append(f'Field {field} value {value} is above maximum {max_val}')
    
    # „Éë„Çø„Éº„É≥Ê§úË®ºÔºàÊ≠£Ë¶èË°®ÁèæÔºâ
    pattern_validations = validation_rules.get('pattern_validations', {})
    for field, pattern in pattern_validations.items():
        if field in item and item[field] is not None:
            import re
            if not re.match(pattern, str(item[field])):
                result['valid'] = False
                result['errors'].append(f'Field {field} does not match pattern {pattern}')
    
    return result

def transform_data_item(item: Dict[str, Any], parameters: Dict[str, Any]) -> Dict[str, Any]:
    """„Éá„Éº„Çø„Ç¢„Ç§„ÉÜ„É†„ÅÆÂ§âÊèõ"""
    
    transformations = parameters.get('transformations', {})
    
    transformed_item = item.copy()
    
    # „Éï„Ç£„Éº„É´„Éâ„ÅÆÊ≠£Ë¶èÂåñ
    if 'normalize_fields' in transformations:
        for field in transformations['normalize_fields']:
            if field in transformed_item and isinstance(transformed_item[field], str):
                transformed_item[field] = transformed_item[field].lower().strip()
    
    # Ë®àÁÆó„Éï„Ç£„Éº„É´„Éâ„ÅÆËøΩÂä†
    if 'calculated_fields' in transformations:
        for calc_field, expression in transformations['calculated_fields'].items():
            try:
                # ÂÆâÂÖ®„Å™Ë®àÁÆóÂºè„ÅÆË©ï‰æ°
                safe_dict = {k: v for k, v in transformed_item.items() if isinstance(k, str)}
                transformed_item[calc_field] = eval(expression, {"__builtins__": {}}, safe_dict)
            except Exception as e:
                transformed_item[calc_field] = None
                logger.warning(f"Failed to calculate field {calc_field}: {e}")
    
    # „Éï„Ç£„Éº„É´„Éâ„ÅÆÂ§âÊèõ
    if 'field_mappings' in transformations:
        for old_field, new_field in transformations['field_mappings'].items():
            if old_field in transformed_item:
                transformed_item[new_field] = transformed_item[old_field]
                if old_field != new_field:
                    del transformed_item[old_field]
    
    # „Éá„Éº„ÇøÂûãÂ§âÊèõ
    if 'type_conversions' in transformations:
        for field, target_type in transformations['type_conversions'].items():
            if field in transformed_item and transformed_item[field] is not None:
                try:
                    if target_type == 'int':
                        transformed_item[field] = int(transformed_item[field])
                    elif target_type == 'float':
                        transformed_item[field] = float(transformed_item[field])
                    elif target_type == 'str':
                        transformed_item[field] = str(transformed_item[field])
                    elif target_type == 'bool':
                        transformed_item[field] = bool(transformed_item[field])
                except (ValueError, TypeError) as e:
                    logger.warning(f"Failed to convert {field} to {target_type}: {e}")
    
    return transformed_item

def analyze_data_item(item: Dict[str, Any], parameters: Dict[str, Any]) -> Dict[str, Any]:
    """„Éá„Éº„Çø„Ç¢„Ç§„ÉÜ„É†„ÅÆÂàÜÊûê"""
    
    analysis_config = parameters.get('analysis_config', {})
    
    analysis_result = {
        'item_id': item.get('id'),
        'metrics': {},
        'categories': [],
        'anomalies': [],
        'insights': []
    }
    
    # Êï∞ÂÄ§„É°„Éà„É™„ÇØ„Çπ„ÅÆË®àÁÆó
    numeric_fields = analysis_config.get('numeric_fields', [])
    for field in numeric_fields:
        if field in item and isinstance(item[field], (int, float)):
            analysis_result['metrics'][field] = {
                'value': item[field],
                'normalized': item[field] / analysis_config.get('normalization_factors', {}).get(field, 1),
                'percentile': calculate_percentile(item[field], field, analysis_config)
            }
    
    # „Ç´„ÉÜ„Ç¥„É™ÂàÜÈ°û
    categorization_rules = analysis_config.get('categorization_rules', {})
    for category, rules in categorization_rules.items():
        if evaluate_categorization_rules(item, rules):
            analysis_result['categories'].append(category)
    
    # Áï∞Â∏∏Ê§úÁü•
    anomaly_detection = analysis_config.get('anomaly_detection', {})
    if anomaly_detection.get('enabled', False):
        anomalies = detect_anomalies(item, anomaly_detection)
        analysis_result['anomalies'].extend(anomalies)
    
    return analysis_result

def aggregate_data_item(item: Dict[str, Any], parameters: Dict[str, Any]) -> Dict[str, Any]:
    """„Éá„Éº„Çø„Ç¢„Ç§„ÉÜ„É†„ÅÆÈõÜÁ¥Ñ"""
    
    aggregation_config = parameters.get('aggregation_config', {})
    
    # „Ç∞„É´„Éº„Éó„Ç≠„Éº„ÅÆÁîüÊàê
    group_fields = aggregation_config.get('group_by', [])
    group_key = tuple(item.get(field, 'null') for field in group_fields)
    
    # ÈõÜÁ¥ÑÂÄ§„ÅÆË®àÁÆó
    aggregate_fields = aggregation_config.get('aggregate_fields', {})
    aggregated_values = {}
    
    for field, aggregation_type in aggregate_fields.items():
        if field in item:
            value = item[field]
            
            if aggregation_type == 'sum':
                aggregated_values[f'{field}_sum'] = value
            elif aggregation_type == 'count':
                aggregated_values[f'{field}_count'] = 1
            elif aggregation_type == 'avg':
                aggregated_values[f'{field}_sum'] = value
                aggregated_values[f'{field}_count'] = 1
            elif aggregation_type == 'min':
                aggregated_values[f'{field}_min'] = value
            elif aggregation_type == 'max':
                aggregated_values[f'{field}_max'] = value
    
    return {
        'group_key': group_key,
        'aggregated_values': aggregated_values,
        'original_item': item
    }
```

## ‰ΩøÁî®‰æã„Å®ÊúÄÈÅ©Âåñ

### ÂàÜÊï£Âá¶ÁêÜÂÆüË°å‰æã

```python
# ÂàÜÊï£Âá¶ÁêÜ„ÅÆÂÆüË°å‰æã
distributed_processor = DistributedProcessor(app)

def run_distributed_processing_example():
    """ÂàÜÊï£Âá¶ÁêÜ„ÅÆÂÆüË°å‰æã"""
    
    # „Éá„Éº„ÇøÊ§úË®º„ÅÆÂàÜÊï£Âá¶ÁêÜ
    validation_job = distributed_processor.process_large_dataset(
        dataset_id="dataset_001",
        processing_function="data_validation",
        parameters={
            "validation_rules": {
                "required_fields": ["id", "name", "email"],
                "type_validations": {
                    "id": int,
                    "name": str,
                    "email": str
                },
                "pattern_validations": {
                    "email": r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
                }
            }
        },
        partition_size=5000
    )
    
    # „Éá„Éº„ÇøÂ§âÊèõ„ÅÆÂàÜÊï£Âá¶ÁêÜ
    transformation_job = distributed_processor.process_large_dataset(
        dataset_id="dataset_001",
        processing_function="data_transformation",
        parameters={
            "transformations": {
                "normalize_fields": ["name", "email"],
                "calculated_fields": {
                    "name_length": "len(name) if 'name' in locals() and name else 0"
                },
                "type_conversions": {
                    "age": "int",
                    "score": "float"
                }
            }
        },
        partition_size=5000
    )
    
    print(f"Started validation job: {validation_job}")
    print(f"Started transformation job: {transformation_job}")
    
    return validation_job, transformation_job

def monitor_distributed_jobs():
    """ÂàÜÊï£„Ç∏„Éß„Éñ„ÅÆÁõ£Ë¶ñ‰æã"""
    
    # ÂÆüË°å‰∏≠„ÅÆ„Ç∏„Éß„ÉñÁõ£Ë¶ñ
    while distributed_processor.active_jobs:
        for job_id in list(distributed_processor.active_jobs.keys()):
            status = distributed_processor.get_job_status(job_id)
            
            print(f"Job {job_id}: {status['status']}")
            
            if 'progress' in status:
                progress = status['progress']
                print(f"  Progress: {progress['percentage']}% ({progress['completed_partitions']}/{progress['total_partitions']})")
            
            if status['status'] in ['completed', 'failed']:
                if status['status'] == 'completed':
                    print(f"  Duration: {status['duration']:.2f}s")
                    print(f"  Result summary: {status['result'].get('total_processed', 'N/A')} items")
                else:
                    print(f"  Error: {status['error']}")
        
        time.sleep(5)
    
    # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÁµ±Ë®à„ÅÆË°®Á§∫
    stats = distributed_processor.get_performance_stats()
    print("\nPerformance Statistics:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
```

### „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊúÄÈÅ©Âåñ„ÅÆ„Éí„É≥„Éà

```python
class OptimizedDistributedProcessor(DistributedProcessor):
    """ÊúÄÈÅ©Âåñ„Åï„Çå„ÅüÂàÜÊï£Âá¶ÁêÜ„Ç∑„Çπ„ÉÜ„É†"""
    
    def __init__(self, celery_app):
        super().__init__(celery_app)
        self.optimization_config = {
            'dynamic_partitioning': True,
            'adaptive_worker_count': True,
            'load_balancing': True,
            'caching_enabled': True
        }
    
    def _create_optimized_partitions(self, dataset_id: str, dataset_size: int, base_partition_size: int):
        """ÊúÄÈÅ©Âåñ„Åï„Çå„Åü„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥‰ΩúÊàê"""
        
        if not self.optimization_config['dynamic_partitioning']:
            return super()._create_partitions(dataset_id, dataset_size, base_partition_size)
        
        # „Ç∑„Çπ„ÉÜ„É†„É™„ÇΩ„Éº„Çπ„Å´Âü∫„Å•„ÅèÂãïÁöÑ„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„Çµ„Ç§„Ç∫Ë™øÊï¥
        optimal_partition_size = self._calculate_optimal_partition_size(dataset_size, base_partition_size)
        
        return super()._create_partitions(dataset_id, dataset_size, optimal_partition_size)
    
    def _calculate_optimal_partition_size(self, dataset_size: int, base_size: int) -> int:
        """ÊúÄÈÅ©„Å™„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„Çµ„Ç§„Ç∫„ÅÆË®àÁÆó"""
        
        try:
            import psutil
            
            # „É°„É¢„É™‰ΩøÁî®Èáè„Å´Âü∫„Å•„ÅèË™øÊï¥
            memory_percent = psutil.virtual_memory().percent
            if memory_percent > 80:
                return max(base_size // 2, 1000)  # „É°„É¢„É™‰∏çË∂≥ÊôÇ„ÅØÂ∞è„Åï„Åè
            elif memory_percent < 50:
                return min(base_size * 2, 50000)  # „É°„É¢„É™‰ΩôË£ïÊôÇ„ÅØÂ§ß„Åç„Åè
            
        except ImportError:
            pass
        
        return base_size
```