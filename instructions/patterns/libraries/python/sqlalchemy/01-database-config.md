# SQLAlchemy 2.0 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š

SQLAlchemy 2.0ã«ãŠã‘ã‚‹éžåŒæœŸã‚¨ãƒ³ã‚¸ãƒ³ã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®šã€‚ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒã§ã®æœ€é©åŒ–ã€ç›£è¦–ã€ã‚«ã‚¹ã‚¿ãƒ åž‹å®šç¾©ã‚’é‡è¦–ã—ãŸå®Ÿè£…æŒ‡é‡ã€‚

## ðŸ”§ SQLAlchemy 2.0 åŸºæœ¬è¨­å®š

### éžåŒæœŸã‚¨ãƒ³ã‚¸ãƒ³ã¨ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š

```python
# database/config.py
from sqlalchemy.ext.asyncio import (
    create_async_engine, 
    AsyncSession, 
    async_sessionmaker,
    AsyncEngine
)
from sqlalchemy.orm import DeclarativeBase, MappedAsDataclass
from sqlalchemy.pool import NullPool, QueuePool
from sqlalchemy import event, text
from contextlib import asynccontextmanager
from typing import AsyncGenerator, Optional
import logging

from config.settings import settings


class Base(MappedAsDataclass, DeclarativeBase):
    """
    SQLAlchemy 2.0 ã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹
    MappedAsDataclassã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹æ©Ÿèƒ½ã‚’æä¾›
    """
    pass


class DatabaseManager:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŽ¥ç¶šã¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†"""
    
    def __init__(self):
        self.engine: Optional[AsyncEngine] = None
        self.session_factory: Optional[async_sessionmaker[AsyncSession]] = None
        
    async def initialize(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        # éžåŒæœŸã‚¨ãƒ³ã‚¸ãƒ³ä½œæˆ
        if settings.ENVIRONMENT == "test":
            # ãƒ†ã‚¹ãƒˆç’°å¢ƒ: ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã¾ãŸã¯ä¸€æ™‚çš„ãªæŽ¥ç¶š
            self.engine = create_async_engine(
                settings.DATABASE_URL,
                echo=settings.DATABASE_ECHO,
                poolclass=NullPool,  # ãƒ†ã‚¹ãƒˆæ™‚ã¯æŽ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚’ç„¡åŠ¹åŒ–
                isolation_level="AUTOCOMMIT",
            )
        else:
            # ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒ: æœ€é©åŒ–ã•ã‚ŒãŸæŽ¥ç¶šãƒ—ãƒ¼ãƒ«
            self.engine = create_async_engine(
                settings.DATABASE_URL,
                echo=settings.DATABASE_ECHO,
                poolclass=QueuePool,
                pool_size=settings.DATABASE_POOL_SIZE,
                max_overflow=settings.DATABASE_MAX_OVERFLOW,
                pool_timeout=settings.DATABASE_POOL_TIMEOUT,
                pool_pre_ping=True,  # æŽ¥ç¶šã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
                pool_recycle=3600,   # 1æ™‚é–“ã§æŽ¥ç¶šã‚’ãƒªã‚µã‚¤ã‚¯ãƒ«
                connect_args={
                    "command_timeout": 60,
                    "server_settings": {
                        "application_name": settings.APP_NAME,
                        "jit": "off",  # JITã‚’ç„¡åŠ¹åŒ–ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
                    }
                }
            )
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ä½œæˆ
        self.session_factory = async_sessionmaker(
            bind=self.engine,
            class_=AsyncSession,
            expire_on_commit=False,  # ã‚³ãƒŸãƒƒãƒˆå¾Œã‚‚ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½¿ç”¨å¯èƒ½
            autoflush=True,          # è‡ªå‹•ãƒ•ãƒ©ãƒƒã‚·ãƒ¥æœ‰åŠ¹
            autocommit=False,        # è‡ªå‹•ã‚³ãƒŸãƒƒãƒˆç„¡åŠ¹
        )
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŽ¥ç¶šãƒ†ã‚¹ãƒˆ
        await self._test_connection()
        
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼è¨­å®š
        self._setup_event_listeners()
        
        logging.info("Database initialized successfully")
    
    async def _test_connection(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŽ¥ç¶šãƒ†ã‚¹ãƒˆ"""
        try:
            async with self.engine.begin() as conn:
                await conn.execute(text("SELECT 1"))
            logging.info("Database connection test successful")
        except Exception as e:
            logging.error(f"Database connection failed: {e}")
            raise
    
    def _setup_event_listeners(self):
        """SQLAlchemyã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼è¨­å®š"""
        
        @event.listens_for(self.engine.sync_engine, "connect")
        def set_postgresql_options(dbapi_connection, connection_record):
            """PostgreSQLæœ€é©åŒ–è¨­å®š"""
            if "postgresql" in settings.DATABASE_URL:
                with dbapi_connection.cursor() as cursor:
                    # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³è¨­å®š
                    cursor.execute("SET timezone TO 'UTC'")
                    # åˆ†é›¢ãƒ¬ãƒ™ãƒ«è¨­å®š
                    cursor.execute("SET default_transaction_isolation TO 'read committed'")
                    # çµ±è¨ˆæƒ…å ±æ›´æ–°
                    cursor.execute("SET track_counts TO on")
                    cursor.execute("SET track_io_timing TO on")
        
        @event.listens_for(AsyncSession, "before_bulk_insert")
        def receive_before_bulk_insert(query, query_context, result):
            """ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå‰ã®ãƒ­ã‚°"""
            logging.info(f"Bulk insert starting: {query}")
        
        @event.listens_for(AsyncSession, "after_transaction_end")
        def receive_after_transaction_end(session, transaction):
            """ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³çµ‚äº†å¾Œã®ãƒ­ã‚°"""
            if transaction.is_active:
                logging.debug("Transaction committed successfully")
    
    async def create_tables(self):
        """ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ"""
        async with self.engine.begin() as conn:
            await conn.run_sync(Base.metadata.create_all)
        logging.info("Tables created successfully")
    
    async def drop_tables(self):
        """ãƒ†ãƒ¼ãƒ–ãƒ«å‰Šé™¤"""
        async with self.engine.begin() as conn:
            await conn.run_sync(Base.metadata.drop_all)
        logging.info("Tables dropped successfully")
    
    async def close(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŽ¥ç¶šçµ‚äº†"""
        if self.engine:
            await self.engine.dispose()
        logging.info("Database connections closed")


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
db_manager = DatabaseManager()


# ä¾å­˜æ€§æ³¨å…¥ç”¨
async def get_async_session() -> AsyncGenerator[AsyncSession, None]:
    """FastAPIä¾å­˜æ€§æ³¨å…¥ç”¨ã‚»ãƒƒã‚·ãƒ§ãƒ³å–å¾—"""
    async with db_manager.get_session() as session:
        yield session
```

### ã‚«ã‚¹ã‚¿ãƒ åž‹å®šç¾©

```python
# database/types.py
from sqlalchemy import TypeDecorator, String, Text
from sqlalchemy.dialects.postgresql import UUID, JSONB, ARRAY
from typing import Any, Optional, Dict, List
import json
import uuid


class GUID(TypeDecorator):
    """ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ç‹¬ç«‹ãªGUIDåž‹"""
    impl = String
    cache_ok = True
    
    def load_dialect_impl(self, dialect):
        if dialect.name == 'postgresql':
            return dialect.type_descriptor(UUID())
        else:
            return dialect.type_descriptor(String(36))
    
    def process_bind_param(self, value, dialect):
        if value is None:
            return value
        elif dialect.name == 'postgresql':
            return str(value)
        else:
            if not isinstance(value, uuid.UUID):
                return str(uuid.UUID(value))
            return str(value)
    
    def process_result_value(self, value, dialect):
        if value is None:
            return value
        else:
            if not isinstance(value, uuid.UUID):
                return uuid.UUID(value)
            return value


class JSONType(TypeDecorator):
    """ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ç‹¬ç«‹ãªJSONåž‹"""
    impl = Text
    cache_ok = True
    
    def load_dialect_impl(self, dialect):
        if dialect.name == 'postgresql':
            return dialect.type_descriptor(JSONB())
        else:
            return dialect.type_descriptor(Text())
    
    def process_bind_param(self, value, dialect):
        if value is None:
            return value
        if dialect.name == 'postgresql':
            return value
        else:
            return json.dumps(value)
    
    def process_result_value(self, value, dialect):
        if value is None:
            return value
        if dialect.name == 'postgresql':
            return value
        else:
            return json.loads(value)


class ArrayType(TypeDecorator):
    """ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ç‹¬ç«‹ãªé…åˆ—åž‹"""
    impl = Text
    cache_ok = True
    
    def __init__(self, item_type=String):
        self.item_type = item_type
        super().__init__()
    
    def load_dialect_impl(self, dialect):
        if dialect.name == 'postgresql':
            return dialect.type_descriptor(ARRAY(self.item_type))
        else:
            return dialect.type_descriptor(Text())
    
    def process_bind_param(self, value, dialect):
        if value is None:
            return value
        if dialect.name == 'postgresql':
            return value
        else:
            return json.dumps(value)
    
    def process_result_value(self, value, dialect):
        if value is None:
            return value
        if dialect.name == 'postgresql':
            return value
        else:
            return json.loads(value)
```

## ðŸ”§ æŽ¥ç¶šãƒ—ãƒ¼ãƒ«æœ€é©åŒ–

### ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒã®æŽ¥ç¶šãƒ—ãƒ¼ãƒ«è¨­å®š

```python
# config/database_settings.py
from pydantic import BaseSettings
from typing import Optional

class DatabaseSettings(BaseSettings):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š"""
    DATABASE_URL: str
    DATABASE_ECHO: bool = False
    DATABASE_POOL_SIZE: int = 10
    DATABASE_MAX_OVERFLOW: int = 20
    DATABASE_POOL_TIMEOUT: int = 30
    DATABASE_POOL_RECYCLE: int = 3600
    DATABASE_POOL_PRE_PING: bool = True
    
    # é«˜è² è·ç’°å¢ƒã§ã®è¨­å®š
    DATABASE_CONNECT_TIMEOUT: int = 60
    DATABASE_COMMAND_TIMEOUT: int = 60
    
    class Config:
        env_file = ".env"


# ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³æœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³
def create_optimized_engine(settings: DatabaseSettings):
    """é«˜æ€§èƒ½ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒç”¨ã‚¨ãƒ³ã‚¸ãƒ³"""
    return create_async_engine(
        settings.DATABASE_URL,
        echo=settings.DATABASE_ECHO,
        poolclass=QueuePool,
        pool_size=settings.DATABASE_POOL_SIZE,
        max_overflow=settings.DATABASE_MAX_OVERFLOW,
        pool_timeout=settings.DATABASE_POOL_TIMEOUT,
        pool_recycle=settings.DATABASE_POOL_RECYCLE,
        pool_pre_ping=settings.DATABASE_POOL_PRE_PING,
        # PostgreSQLå›ºæœ‰ã®æœ€é©åŒ–
        connect_args={
            "command_timeout": settings.DATABASE_COMMAND_TIMEOUT,
            "server_settings": {
                "application_name": "MyApp",
                "tcp_keepalives_idle": "600",
                "tcp_keepalives_interval": "30",
                "tcp_keepalives_count": "3",
            }
        },
        # éžåŒæœŸå‡¦ç†æœ€é©åŒ–
        execution_options={
            "isolation_level": "READ_COMMITTED",
            "autocommit": False,
        }
    )
```

## ðŸ“Š ç›£è¦–ã¨ãƒ­ã‚®ãƒ³ã‚°

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŽ¥ç¶šç›£è¦–

```python
# monitoring/database_monitor.py
import time
import logging
from sqlalchemy import event
from sqlalchemy.engine import Engine
from sqlalchemy.pool import Pool

logger = logging.getLogger(__name__)

class DatabaseMonitor:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŽ¥ç¶šç›£è¦–"""
    
    def __init__(self):
        self.connection_count = 0
        self.query_count = 0
        self.slow_query_threshold = 1.0  # 1ç§’
    
    def setup_monitoring(self, engine: Engine):
        """ç›£è¦–è¨­å®š"""
        
        @event.listens_for(Pool, "connect")
        def receive_connect(dbapi_connection, connection_record):
            """æŽ¥ç¶šæ™‚ã®ç›£è¦–"""
            self.connection_count += 1
            logger.info(f"Database connection established. Total: {self.connection_count}")
        
        @event.listens_for(Pool, "checkout")
        def receive_checkout(dbapi_connection, connection_record, connection_proxy):
            """æŽ¥ç¶šãƒã‚§ãƒƒã‚¯ã‚¢ã‚¦ãƒˆæ™‚ã®ç›£è¦–"""
            logger.debug("Connection checked out from pool")
        
        @event.listens_for(Pool, "checkin")
        def receive_checkin(dbapi_connection, connection_record):
            """æŽ¥ç¶šãƒã‚§ãƒƒã‚¯ã‚¤ãƒ³æ™‚ã®ç›£è¦–"""
            logger.debug("Connection checked in to pool")
        
        @event.listens_for(Engine, "before_cursor_execute")
        def receive_before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
            """ã‚¯ã‚¨ãƒªå®Ÿè¡Œå‰ã®ç›£è¦–"""
            context._query_start_time = time.time()
            self.query_count += 1
        
        @event.listens_for(Engine, "after_cursor_execute")
        def receive_after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
            """ã‚¯ã‚¨ãƒªå®Ÿè¡Œå¾Œã®ç›£è¦–"""
            total = time.time() - context._query_start_time
            if total > self.slow_query_threshold:
                logger.warning(f"Slow query detected: {total:.2f}s - {statement[:100]}")
            logger.debug(f"Query executed in {total:.3f}s")


# ä½¿ç”¨ä¾‹
monitor = DatabaseMonitor()
monitor.setup_monitoring(engine)
```