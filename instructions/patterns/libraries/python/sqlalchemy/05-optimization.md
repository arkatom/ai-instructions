# SQLAlchemy 2.0 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æœ€é©åŒ–

SQLAlchemy 2.0ã«ãŠã‘ã‚‹é«˜åº¦ãªæœ€é©åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã€‚N+1å•é¡Œã®è§£æ±ºã€ãƒãƒ«ã‚¯æ“ä½œã€ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥ã€ã‚¯ã‚¨ãƒªæœ€é©åŒ–ã®å®Ÿè£…ã€‚

## âš¡ N+1å•é¡Œã®å¾¹åº•è§£æ±º

### selectinloadã«ã‚ˆã‚‹æœ€é©åŒ–

```python
# repositories/optimized_loading.py
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, text
from sqlalchemy.orm import selectinload, joinedload, contains_eager, Load
from typing import List, Dict, Any
from datetime import datetime


class OptimizedLoadingRepository:
    """N+1å•é¡Œã‚’è§£æ±ºã™ã‚‹ãƒªãƒã‚¸ãƒˆãƒª"""
    
    def __init__(self, session: AsyncSession):
        self.session = session
    
    async def get_posts_with_everything_optimized(
        self,
        limit: int = 20
    ) -> List[Dict[str, Any]]:
        """å…¨é–¢é€£ãƒ‡ãƒ¼ã‚¿ã‚’åŠ¹çŽ‡çš„ã«å–å¾—"""
        
        stmt = (
            select(Post)
            .options(
                # è‘—è€…æƒ…å ±ï¼ˆ1å¯¾1ï¼‰
                selectinload(Post.author),
                
                # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ï¼ˆå¤šå¯¾1ï¼‰
                selectinload(Post.category),
                
                # ã‚¿ã‚°æƒ…å ±ï¼ˆå¤šå¯¾å¤šï¼‰
                selectinload(Post.tags),
                
                # ã‚³ãƒ¡ãƒ³ãƒˆã¨è‘—è€…ã‚’ä¸€åº¦ã«å–å¾—
                selectinload(Post.comments.and_(
                    Comment.is_deleted == False
                )).selectinload(Comment.author),
                
                # ãƒã‚¹ãƒˆã—ãŸã‚³ãƒ¡ãƒ³ãƒˆ
                selectinload(Post.comments).selectinload(Comment.replies.and_(
                    Comment.is_deleted == False
                )).selectinload(Comment.author)
            )
            .where(Post.is_published == True)
            .where(Post.is_deleted == False)
            .order_by(Post.created_at.desc())
            .limit(limit)
        )
        
        result = await self.session.execute(stmt)
        posts = result.scalars().all()
        
        # è¿½åŠ ã®é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åº¦ã®ã‚¯ã‚¨ãƒªã§å–å¾—
        post_ids = [post.id for post in posts]
        
        stats_stmt = (
            select(
                Post.id,
                func.count(Comment.id).label("comment_count"),
                func.count(PostLike.id).label("like_count"),
                func.count(func.distinct(Comment.author_id)).label("unique_commenters")
            )
            .outerjoin(Comment, and_(
                Comment.post_id == Post.id,
                Comment.is_deleted == False
            ))
            .outerjoin(PostLike)
            .where(Post.id.in_(post_ids))
            .group_by(Post.id)
        )
        
        stats_result = await self.session.execute(stats_stmt)
        stats_by_post_id = {
            row.id: {
                "comment_count": row.comment_count,
                "like_count": row.like_count,
                "unique_commenters": row.unique_commenters
            }
            for row in stats_result
        }
        
        # çµæžœã‚’ãƒžãƒ¼ã‚¸
        posts_with_stats = []
        for post in posts:
            stats = stats_by_post_id.get(post.id, {
                "comment_count": 0,
                "like_count": 0,
                "unique_commenters": 0
            })
            
            posts_with_stats.append({
                "post": post,
                "stats": stats
            })
        
        return posts_with_stats
    
    async def get_user_feed_optimized(
        self,
        user_id: int,
        limit: int = 20
    ) -> List[Dict[str, Any]]:
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰"""
        
        # ãƒ•ã‚©ãƒ­ãƒ¼ä¸­ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å–å¾—
        following_stmt = (
            select(UserFollow.following_id)
            .where(UserFollow.follower_id == user_id)
        )
        following_result = await self.session.execute(following_stmt)
        following_ids = [row[0] for row in following_result]
        
        if not following_ids:
            return []
        
        # æŠ•ç¨¿ã‚’åŠ¹çŽ‡çš„ã«å–å¾—
        stmt = (
            select(Post)
            .options(
                selectinload(Post.author),
                selectinload(Post.category),
                selectinload(Post.tags),
                # æœ€æ–°ã®ã‚³ãƒ¡ãƒ³ãƒˆ3ä»¶ã®ã¿
                selectinload(Post.comments.and_(
                    Comment.is_deleted == False
                ).limit(3)).selectinload(Comment.author)
            )
            .where(Post.author_id.in_(following_ids))
            .where(Post.is_published == True)
            .where(Post.is_deleted == False)
            .order_by(Post.created_at.desc())
            .limit(limit)
        )
        
        result = await self.session.execute(stmt)
        posts = result.scalars().all()
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã„ã„ã­çŠ¶æ…‹ã‚’ä¸€æ‹¬å–å¾—
        post_ids = [post.id for post in posts]
        liked_posts_stmt = (
            select(PostLike.post_id)
            .where(PostLike.user_id == user_id)
            .where(PostLike.post_id.in_(post_ids))
        )
        liked_result = await self.session.execute(liked_posts_stmt)
        liked_post_ids = set(row[0] for row in liked_result)
        
        feed_items = []
        for post in posts:
            feed_items.append({
                "post": post,
                "is_liked": post.id in liked_post_ids,
                "comment_preview": post.comments[:3]  # æœ€æ–°3ä»¶
            })
        
        return feed_items
```

## ðŸš€ ãƒãƒ«ã‚¯æ“ä½œã®æœ€é©åŒ–

### é«˜é€Ÿãƒãƒ«ã‚¯å‡¦ç†ãƒ‘ã‚¿ãƒ¼ãƒ³

```python
# repositories/bulk_operations.py
from sqlalchemy import select, update, delete, text, func, bindparam
from sqlalchemy.dialects.postgresql import insert
from typing import List, Dict, Any


class BulkOperationsRepository:
    """ãƒãƒ«ã‚¯æ“ä½œå°‚ç”¨ãƒªãƒã‚¸ãƒˆãƒª"""
    
    def __init__(self, session: AsyncSession):
        self.session = session
    
    async def bulk_insert_users(self, users_data: List[Dict[str, Any]]) -> List[int]:
        """é«˜é€Ÿãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ"""
        
        # PostgreSQL UPSERTã‚’ä½¿ç”¨
        stmt = insert(User).values(users_data)
        
        # é‡è¤‡æ™‚ã®æ›´æ–°å‡¦ç†
        stmt = stmt.on_conflict_do_update(
            index_elements=['email'],
            set_={
                'updated_at': func.now(),
                'first_name': stmt.excluded.first_name,
                'last_name': stmt.excluded.last_name,
                'display_name': stmt.excluded.display_name
            }
        ).returning(User.id)
        
        result = await self.session.execute(stmt)
        await self.session.commit()
        
        return [row.id for row in result]
    
    async def bulk_update_post_stats(
        self,
        post_updates: List[Dict[str, Any]]
    ) -> int:
        """æŠ•ç¨¿çµ±è¨ˆã®ä¸€æ‹¬æ›´æ–°"""
        
        # ãƒã‚¤ãƒ³ãƒ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸåŠ¹çŽ‡çš„ãªæ›´æ–°
        stmt = (
            update(Post)
            .where(Post.id == bindparam('post_id'))
            .values(
                view_count=bindparam('view_count'),
                like_count=bindparam('like_count'),
                updated_at=func.now()
            )
        )
        
        result = await self.session.execute(stmt, post_updates)
        affected_rows = result.rowcount
        await self.session.commit()
        
        return affected_rows
    
    async def bulk_soft_delete_posts(self, post_ids: List[int]) -> int:
        """æŠ•ç¨¿ã®ä¸€æ‹¬è«–ç†å‰Šé™¤"""
        
        stmt = (
            update(Post)
            .where(Post.id.in_(post_ids))
            .values(
                is_deleted=True,
                deleted_at=func.now()
            )
        )
        
        result = await self.session.execute(stmt)
        affected_rows = result.rowcount
        
        # é–¢é€£ã‚³ãƒ¡ãƒ³ãƒˆã‚‚è«–ç†å‰Šé™¤
        comment_stmt = (
            update(Comment)
            .where(Comment.post_id.in_(post_ids))
            .values(
                is_deleted=True,
                deleted_at=func.now()
            )
        )
        
        await self.session.execute(comment_stmt)
        await self.session.commit()
        
        return affected_rows
    
    async def bulk_create_user_follows(
        self,
        follower_id: int,
        following_ids: List[int]
    ) -> int:
        """ãƒ•ã‚©ãƒ­ãƒ¼é–¢ä¿‚ã®ä¸€æ‹¬ä½œæˆ"""
        
        follow_data = [
            {"follower_id": follower_id, "following_id": following_id}
            for following_id in following_ids
        ]
        
        # é‡è¤‡ã‚’é¿ã‘ã‚‹UPSERT
        stmt = insert(UserFollow).values(follow_data)
        stmt = stmt.on_conflict_do_nothing(
            index_elements=['follower_id', 'following_id']
        )
        
        result = await self.session.execute(stmt)
        await self.session.commit()
        
        return result.rowcount
    
    async def bulk_analyze_user_engagement(self, user_ids: List[int]) -> Dict[int, Dict[str, Any]]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã®ä¸€æ‹¬åˆ†æž"""
        
        # CTEã‚’ä½¿ç”¨ã—ãŸåŠ¹çŽ‡çš„ãªé›†è¨ˆ
        stmt = text("""
            WITH user_stats AS (
                SELECT 
                    u.id as user_id,
                    COUNT(DISTINCT p.id) as post_count,
                    COUNT(DISTINCT c.id) as comment_count,
                    COUNT(DISTINCT pl.id) as received_likes,
                    AVG(p.view_count) as avg_views_per_post,
                    MAX(p.created_at) as last_post_date
                FROM users u
                LEFT JOIN posts p ON u.id = p.author_id AND p.is_deleted = false
                LEFT JOIN comments c ON u.id = c.author_id AND c.is_deleted = false
                LEFT JOIN post_likes pl ON p.id = pl.post_id
                WHERE u.id = ANY(:user_ids)
                GROUP BY u.id
            )
            SELECT 
                user_id,
                post_count,
                comment_count,
                received_likes,
                COALESCE(avg_views_per_post, 0) as avg_views_per_post,
                last_post_date,
                CASE 
                    WHEN post_count = 0 THEN 0
                    ELSE received_likes::float / post_count 
                END as likes_per_post
            FROM user_stats
        """)
        
        result = await self.session.execute(stmt, {"user_ids": user_ids})
        
        engagement_data = {}
        for row in result:
            engagement_data[row.user_id] = {
                "post_count": row.post_count,
                "comment_count": row.comment_count,
                "received_likes": row.received_likes,
                "avg_views_per_post": float(row.avg_views_per_post or 0),
                "last_post_date": row.last_post_date,
                "likes_per_post": float(row.likes_per_post or 0)
            }
        
        return engagement_data
```

## ðŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥

### Redisçµ±åˆã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°

```python
# services/caching_service.py
import redis.asyncio as redis
import json
import pickle
from typing import Any, Optional, Dict, List
from datetime import timedelta
import hashlib


class CachingService:
    """é«˜åº¦ãªã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis = redis.from_url(redis_url, decode_responses=False)
    
    def _make_key(self, prefix: str, **kwargs) -> str:
        """ã‚­ãƒ¼ç”Ÿæˆ"""
        key_data = f"{prefix}:{json.dumps(kwargs, sort_keys=True)}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    async def get_cached_query_result(
        self,
        cache_key: str,
        query_func,
        ttl: int = 300,
        **query_params
    ) -> Any:
        """ã‚¯ã‚¨ãƒªçµæžœã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        
        key = self._make_key(cache_key, **query_params)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—è©¦è¡Œ
        cached = await self.redis.get(key)
        if cached:
            return pickle.loads(cached)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ - ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
        result = await query_func(**query_params)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        serialized = pickle.dumps(result)
        await self.redis.setex(key, ttl, serialized)
        
        return result
    
    async def invalidate_pattern(self, pattern: str):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒžãƒƒãƒã«ã‚ˆã‚‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–"""
        keys = await self.redis.keys(pattern)
        if keys:
            await self.redis.delete(*keys)
    
    async def get_popular_posts_cached(
        self,
        limit: int = 10,
        ttl: int = 600
    ) -> List[Dict[str, Any]]:
        """äººæ°—æŠ•ç¨¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        
        cache_key = f"popular_posts:{limit}"
        cached = await self.redis.get(cache_key)
        
        if cached:
            return json.loads(cached)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒª
        stmt = (
            select(Post, User.username.label("author_username"))
            .join(User, Post.author_id == User.id)
            .where(Post.is_published == True)
            .where(Post.is_deleted == False)
            .order_by((Post.like_count + Post.view_count * 0.1).desc())
            .limit(limit)
        )
        
        result = await self.session.execute(stmt)
        posts_data = []
        
        for row in result:
            posts_data.append({
                "id": row.Post.id,
                "title": row.Post.title,
                "author_username": row.author_username,
                "like_count": row.Post.like_count,
                "view_count": row.Post.view_count,
                "created_at": row.Post.created_at.isoformat()
            })
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        await self.redis.setex(cache_key, ttl, json.dumps(posts_data))
        
        return posts_data


# repositories/cached_repository.py
class CachedUserRepository(UserRepository):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ããƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªãƒã‚¸ãƒˆãƒª"""
    
    def __init__(self, session: AsyncSession, cache_service: CachingService):
        super().__init__(session)
        self.cache = cache_service
    
    async def get_user_with_stats_cached(self, user_id: int) -> Optional[Dict[str, Any]]:
        """çµ±è¨ˆæƒ…å ±ä»˜ããƒ¦ãƒ¼ã‚¶ãƒ¼å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç‰ˆï¼‰"""
        
        async def fetch_user_stats(user_id: int):
            user = await self.get_by_id_with_posts(user_id)
            if not user:
                return None
            
            # çµ±è¨ˆæƒ…å ±å–å¾—
            stats_stmt = (
                select(
                    func.count(Post.id).label("post_count"),
                    func.count(UserFollow.id).label("follower_count"),
                    func.sum(Post.view_count).label("total_views"),
                    func.sum(Post.like_count).label("total_likes")
                )
                .select_from(
                    User
                    .outerjoin(Post, and_(
                        Post.author_id == User.id,
                        Post.is_deleted == False
                    ))
                    .outerjoin(UserFollow, UserFollow.following_id == User.id)
                )
                .where(User.id == user_id)
            )
            
            stats_result = await self.session.execute(stats_stmt)
            stats = stats_result.first()
            
            return {
                "user": user,
                "stats": {
                    "post_count": stats.post_count or 0,
                    "follower_count": stats.follower_count or 0,
                    "total_views": stats.total_views or 0,
                    "total_likes": stats.total_likes or 0
                }
            }
        
        return await self.cache.get_cached_query_result(
            "user_stats",
            fetch_user_stats,
            ttl=300,
            user_id=user_id
        )
    
    async def invalidate_user_cache(self, user_id: int):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç„¡åŠ¹åŒ–"""
        await self.cache.invalidate_pattern(f"user_stats:*user_id*{user_id}*")
```

## ðŸ“Š ã‚¯ã‚¨ãƒªæœ€é©åŒ–ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æˆ¦ç•¥

### é«˜åº¦ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ´»ç”¨

```python
# database/indexes.py
from sqlalchemy import Index, text
from sqlalchemy.dialects.postgresql import GIN, BTREE


class OptimizedIndexes:
    """æœ€é©åŒ–ã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®šç¾©"""
    
    @staticmethod
    def create_advanced_indexes():
        """é«˜åº¦ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ"""
        
        indexes = [
            # è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆæ¤œç´¢ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹å‘ä¸Šï¼‰
            Index(
                'idx_posts_author_published_created',
                'author_id', 'is_published', 'created_at',
                postgresql_where=text('is_deleted = false')
            ),
            
            # éƒ¨åˆ†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆæ¡ä»¶ä»˜ãï¼‰
            Index(
                'idx_posts_published_only',
                'created_at',
                postgresql_where=text('is_published = true AND is_deleted = false')
            ),
            
            # GINã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆå…¨æ–‡æ¤œç´¢ãƒ»é…åˆ—æ¤œç´¢ï¼‰
            Index(
                'idx_posts_tags_gin',
                'tags',
                postgresql_using='gin'
            ),
            
            # å¼ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆè¨ˆç®—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰
            Index(
                'idx_posts_engagement_score',
                text('(like_count * 2 + view_count * 0.1)'),
                postgresql_where=text('is_published = true')
            ),
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¤œç´¢ç”¨è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            Index(
                'idx_users_search_composite',
                'status', 'is_active', 'created_at',
                postgresql_where=text('is_deleted = false')
            )
        ]
        
        return indexes


# database/query_optimization.py
class QueryOptimizer:
    """ã‚¯ã‚¨ãƒªæœ€é©åŒ–ãƒ„ãƒ¼ãƒ«"""
    
    def __init__(self, session: AsyncSession):
        self.session = session
    
    async def analyze_query_performance(self, query_text: str, params: dict = None):
        """ã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹åˆ†æž"""
        
        # EXPLAIN ANALYZEå®Ÿè¡Œ
        explain_query = f"EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) {query_text}"
        
        result = await self.session.execute(text(explain_query), params or {})
        plan = result.scalar()
        
        return {
            "execution_time": plan[0]["Execution Time"],
            "planning_time": plan[0]["Planning Time"],
            "total_cost": plan[0]["Plan"]["Total Cost"],
            "rows": plan[0]["Plan"]["Actual Rows"],
            "plan": plan[0]["Plan"]
        }
    
    async def get_slow_queries_report(self) -> List[Dict[str, Any]]:
        """ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªãƒ¬ãƒãƒ¼ãƒˆ"""
        
        # PostgreSQL pg_stat_statementsæ‹¡å¼µã‚’ä½¿ç”¨
        stmt = text("""
            SELECT 
                query,
                calls,
                total_exec_time,
                mean_exec_time,
                rows,
                100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
            FROM pg_stat_statements 
            WHERE query NOT LIKE '%pg_stat_statements%'
            ORDER BY total_exec_time DESC 
            LIMIT 10
        """)
        
        result = await self.session.execute(stmt)
        
        slow_queries = []
        for row in result:
            slow_queries.append({
                "query": row.query,
                "calls": row.calls,
                "total_time": float(row.total_exec_time),
                "avg_time": float(row.mean_exec_time),
                "rows": row.rows,
                "hit_percent": float(row.hit_percent or 0)
            })
        
        return slow_queries
    
    async def optimize_table_statistics(self, table_names: List[str]):
        """ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆæƒ…å ±ã®æœ€é©åŒ–"""
        
        for table_name in table_names:
            # ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆæ›´æ–°
            await self.session.execute(text(f"ANALYZE {table_name}"))
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½¿ç”¨çŠ¶æ³ç¢ºèª
            usage_stmt = text("""
                SELECT 
                    indexname,
                    idx_tup_read,
                    idx_tup_fetch,
                    idx_scan
                FROM pg_stat_user_indexes 
                WHERE relname = :table_name
            """)
            
            result = await self.session.execute(usage_stmt, {"table_name": table_name})
            
            for row in result:
                if row.idx_scan == 0:  # ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                    print(f"Unused index detected: {row.indexname} on table {table_name}")


# monitoring/performance_monitor.py
class PerformanceMonitor:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ç›£è¦–"""
    
    def __init__(self, session: AsyncSession):
        self.session = session
    
    async def get_connection_stats(self) -> Dict[str, Any]:
        """æŽ¥ç¶šçµ±è¨ˆå–å¾—"""
        
        stmt = text("""
            SELECT 
                state,
                COUNT(*) as connection_count,
                AVG(EXTRACT(EPOCH FROM (now() - state_change))) as avg_duration
            FROM pg_stat_activity 
            WHERE datname = current_database()
            GROUP BY state
        """)
        
        result = await self.session.execute(stmt)
        
        stats = {}
        for row in result:
            stats[row.state or 'unknown'] = {
                "count": row.connection_count,
                "avg_duration": float(row.avg_duration or 0)
            }
        
        return stats
    
    async def get_table_sizes(self) -> List[Dict[str, Any]]:
        """ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±"""
        
        stmt = text("""
            SELECT 
                schemaname,
                tablename,
                pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
                pg_total_relation_size(schemaname||'.'||tablename) as size_bytes
            FROM pg_tables 
            WHERE schemaname = 'public'
            ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
        """)
        
        result = await self.session.execute(stmt)
        
        table_sizes = []
        for row in result:
            table_sizes.append({
                "schema": row.schemaname,
                "table": row.tablename,
                "size": row.size,
                "size_bytes": row.size_bytes
            })
        
        return table_sizes
```