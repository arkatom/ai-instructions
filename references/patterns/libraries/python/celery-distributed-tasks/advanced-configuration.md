# Advanced Celery Configuration

> ğŸ¯ **ç›®çš„**: ä¼æ¥­ãƒ¬ãƒ™ãƒ«ã®Celeryè¨­å®šã¨å‹•çš„ãƒ¯ãƒ¼ã‚«ãƒ¼ç®¡ç†
> 
> ğŸ“Š **å¯¾è±¡**: æœ¬ç•ªç’°å¢ƒã§ã®Celeryè¨­å®šã€è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€ç›£è¦–è¨­å®š
> 
> âš¡ **ç‰¹å¾´**: é«˜å¯ç”¨æ€§è¨­è¨ˆã€ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è€ƒæ…®

## ä¼æ¥­ãƒ¬ãƒ™ãƒ«Celeryè¨­å®š

### åŸºæœ¬è¨­å®šã‚¯ãƒ©ã‚¹

```python
from celery import Celery
from kombu import Queue, Exchange
import os
import logging
from datetime import timedelta
from celery.signals import task_prerun, task_postrun, task_failure
import redis
import json

# Celeryã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®è¨­å®š
class CeleryConfig:
    """Celeryè¨­å®šã‚¯ãƒ©ã‚¹"""
    
    # ãƒ–ãƒ­ãƒ¼ã‚«ãƒ¼è¨­å®š
    broker_url = os.getenv('CELERY_BROKER_URL', 'redis://localhost:6379/0')
    result_backend = os.getenv('CELERY_RESULT_BACKEND', 'redis://localhost:6379/0')
    
    # ã‚¿ã‚¹ã‚¯è¨­å®š
    task_serializer = 'json'
    accept_content = ['json']
    result_serializer = 'json'
    timezone = 'UTC'
    enable_utc = True
    
    # çµæœã®ä¿æŒæœŸé–“
    result_expires = 3600  # 1æ™‚é–“
    
    # ã‚¿ã‚¹ã‚¯ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
    task_routes = {
        'tasks.cpu_intensive.*': {'queue': 'cpu_intensive'},
        'tasks.io_intensive.*': {'queue': 'io_intensive'},
        'tasks.priority.*': {'queue': 'priority'},
        'tasks.scheduled.*': {'queue': 'scheduled'},
    }
    
    # ã‚­ãƒ¥ãƒ¼ã®å®šç¾©
    task_default_queue = 'default'
    task_queues = (
        Queue('default', Exchange('default'), routing_key='default'),
        Queue('cpu_intensive', Exchange('cpu'), routing_key='cpu_intensive',
              queue_arguments={'x-max-priority': 10}),
        Queue('io_intensive', Exchange('io'), routing_key='io_intensive'),
        Queue('priority', Exchange('priority'), routing_key='priority',
              queue_arguments={'x-max-priority': 10}),
        Queue('scheduled', Exchange('scheduled'), routing_key='scheduled'),
    )
    
    # ãƒ¯ãƒ¼ã‚«ãƒ¼è¨­å®š
    worker_prefetch_multiplier = 1
    worker_max_tasks_per_child = 1000
    worker_disable_rate_limits = False
    
    # ç›£è¦–è¨­å®š
    worker_send_task_events = True
    task_send_sent_event = True
    
    # ãƒ“ãƒ¼ãƒˆè¨­å®šï¼ˆå®šæœŸã‚¿ã‚¹ã‚¯ï¼‰
    beat_schedule = {
        'cleanup-expired-sessions': {
            'task': 'tasks.cleanup.cleanup_expired_sessions',
            'schedule': timedelta(hours=1),
            'options': {'queue': 'scheduled'}
        },
        'generate-daily-reports': {
            'task': 'tasks.reports.generate_daily_reports',
            'schedule': crontab(hour=2, minute=0),
            'options': {'queue': 'scheduled'}
        },
        'health-check': {
            'task': 'tasks.monitoring.health_check',
            'schedule': timedelta(minutes=5),
            'options': {'queue': 'priority', 'priority': 9}
        }
    }
    
    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š
    worker_hijack_root_logger = False
    worker_log_color = False

# Celeryã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ä½œæˆ
app = Celery('distributed_tasks')
app.config_from_object(CeleryConfig)
```

### ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚¹ã‚¯åŸºåº•ã‚¯ãƒ©ã‚¹

```python
# ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚¹ã‚¯åŸºåº•ã‚¯ãƒ©ã‚¹
class BaseTask(app.Task):
    """ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚¹ã‚¯åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    def on_failure(self, exc, task_id, args, kwargs, einfo):
        """ã‚¿ã‚¹ã‚¯å¤±æ•—æ™‚ã®å‡¦ç†"""
        logger = logging.getLogger(__name__)
        logger.error(f'Task {task_id} failed: {exc}', exc_info=einfo)
        
        # å¤±æ•—ã®è¨˜éŒ²
        self.record_failure(task_id, str(exc), args, kwargs)
    
    def on_success(self, retval, task_id, args, kwargs):
        """ã‚¿ã‚¹ã‚¯æˆåŠŸæ™‚ã®å‡¦ç†"""
        logger = logging.getLogger(__name__)
        logger.info(f'Task {task_id} succeeded')
        
        # æˆåŠŸã®è¨˜éŒ²
        self.record_success(task_id, retval, args, kwargs)
    
    def record_failure(self, task_id, error, args, kwargs):
        """å¤±æ•—ã®è¨˜éŒ²"""
        failure_data = {
            'task_id': task_id,
            'error': error,
            'args': args,
            'kwargs': kwargs,
            'timestamp': time.time()
        }
        
        # Redis ã«å¤±æ•—ãƒ­ã‚°ã‚’ä¿å­˜
        redis_client = redis.Redis.from_url(CeleryConfig.result_backend)
        redis_client.lpush('task_failures', json.dumps(failure_data))
        redis_client.ltrim('task_failures', 0, 999)  # æœ€æ–°1000ä»¶ã‚’ä¿æŒ
    
    def record_success(self, task_id, result, args, kwargs):
        """æˆåŠŸã®è¨˜éŒ²"""
        success_data = {
            'task_id': task_id,
            'result': result,
            'timestamp': time.time()
        }
        
        redis_client = redis.Redis.from_url(CeleryConfig.result_backend)
        redis_client.lpush('task_successes', json.dumps(success_data))
        redis_client.ltrim('task_successes', 0, 999)

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
app.Task = BaseTask
```

### ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã¨ãƒ¯ãƒ¼ã‚«ãƒ¼ç®¡ç†

```python
# ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
@task_prerun.connect
def task_prerun_handler(sender=None, task_id=None, task=None, args=None, kwargs=None, **kwds):
    """ã‚¿ã‚¹ã‚¯å®Ÿè¡Œå‰ã®å‡¦ç†"""
    logger = logging.getLogger(__name__)
    logger.info(f'Task {task_id} started: {task.name}')

@task_postrun.connect
def task_postrun_handler(sender=None, task_id=None, task=None, args=None, kwargs=None, retval=None, state=None, **kwds):
    """ã‚¿ã‚¹ã‚¯å®Ÿè¡Œå¾Œã®å‡¦ç†"""
    logger = logging.getLogger(__name__)
    logger.info(f'Task {task_id} finished: {state}')

# ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†
from celery.signals import worker_process_init, worker_process_shutdown
import psycopg2
from contextlib import contextmanager

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«
db_pool = None

@worker_process_init.connect
def init_worker(**kwargs):
    """ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹åˆæœŸåŒ–"""
    global db_pool
    logger = logging.getLogger(__name__)
    logger.info('Initializing worker process')
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«ã®åˆæœŸåŒ–
    import psycopg2.pool
    db_pool = psycopg2.pool.ThreadedConnectionPool(
        1, 10,  # æœ€å°ãƒ»æœ€å¤§æ¥ç¶šæ•°
        host=os.getenv('DB_HOST', 'localhost'),
        database=os.getenv('DB_NAME', 'app_db'),
        user=os.getenv('DB_USER', 'app_user'),
        password=os.getenv('DB_PASSWORD', 'password')
    )

@worker_process_shutdown.connect
def shutdown_worker(**kwargs):
    """ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†"""
    global db_pool
    logger = logging.getLogger(__name__)
    logger.info('Shutting down worker process')
    
    if db_pool:
        db_pool.closeall()

@contextmanager
def get_db_connection():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã®å–å¾—"""
    global db_pool
    connection = db_pool.getconn()
    try:
        yield connection
    finally:
        db_pool.putconn(connection)
```

## å‹•çš„ãƒ¯ãƒ¼ã‚«ãƒ¼ç®¡ç†

### è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 

```python
import psutil
import time
from celery import current_app
from celery.worker.control import Panel
from typing import Dict, List, Any
import threading
import os
import signal
import subprocess

class DynamicWorkerManager:
    """å‹•çš„ãƒ¯ãƒ¼ã‚«ãƒ¼ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, celery_app):
        self.celery_app = celery_app
        self.worker_processes = {}
        self.monitoring_active = False
        self.monitor_thread = None
        
        # è¨­å®š
        self.max_workers = os.getenv('MAX_WORKERS', 10)
        self.min_workers = os.getenv('MIN_WORKERS', 2)
        self.cpu_threshold = float(os.getenv('CPU_THRESHOLD', 80.0))
        self.memory_threshold = float(os.getenv('MEMORY_THRESHOLD', 85.0))
        self.queue_threshold = int(os.getenv('QUEUE_THRESHOLD', 100))
        
    def start_monitoring(self):
        """ç›£è¦–ã®é–‹å§‹"""
        self.monitoring_active = True
        self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        self.monitor_thread.start()
        
        # åˆæœŸãƒ¯ãƒ¼ã‚«ãƒ¼ã®èµ·å‹•
        for _ in range(self.min_workers):
            self.start_worker()
    
    def stop_monitoring(self):
        """ç›£è¦–ã®åœæ­¢"""
        self.monitoring_active = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=10)
        
        # å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã®åœæ­¢
        self.stop_all_workers()
    
    def _monitoring_loop(self):
        """ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        while self.monitoring_active:
            try:
                self._check_system_resources()
                self._check_queue_lengths()
                self._cleanup_dead_workers()
                
                time.sleep(30)  # 30ç§’é–“éš”ã§ç›£è¦–
                
            except Exception as e:
                logger.error(f"Worker monitoring error: {e}")
    
    def _check_system_resources(self):
        """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã®ç¢ºèª"""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory_percent = psutil.virtual_memory().percent
        
        active_workers = len([p for p in self.worker_processes.values() if p.poll() is None])
        
        # CPUä½¿ç”¨ç‡ãŒé«˜ãã€ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ãŒæœ€å¤§æœªæº€ã®å ´åˆã¯ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—
        if (cpu_percent < self.cpu_threshold / 2 and 
            memory_percent < self.memory_threshold / 2 and 
            active_workers < self.max_workers):
            
            queue_lengths = self._get_queue_lengths()
            total_queued = sum(queue_lengths.values())
            
            if total_queued > self.queue_threshold:
                self.start_worker()
                logger.info(f"Scaled up: CPU {cpu_percent}%, Memory {memory_percent}%, Queued {total_queued}")
        
        # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡ãŒé«˜ã„ã€ã¾ãŸã¯å‡¦ç†å¾…ã¡ãŒå°‘ãªã„å ´åˆã¯ã‚¹ã‚±ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³
        elif ((cpu_percent > self.cpu_threshold or memory_percent > self.memory_threshold) and 
              active_workers > self.min_workers):
            
            self.stop_worker()
            logger.info(f"Scaled down: CPU {cpu_percent}%, Memory {memory_percent}%")
```

### ã‚­ãƒ¥ãƒ¼ç›£è¦–ã¨ç·Šæ€¥ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

```python
    def _check_queue_lengths(self):
        """ã‚­ãƒ¥ãƒ¼ã®é•·ã•ç¢ºèª"""
        queue_lengths = self._get_queue_lengths()
        
        for queue_name, length in queue_lengths.items():
            if length > self.queue_threshold * 2:  # ç·Šæ€¥ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—
                if len(self.worker_processes) < self.max_workers:
                    self.start_worker(f"emergency_{queue_name}")
                    logger.warning(f"Emergency scale up for queue {queue_name}: {length} tasks")
    
    def _get_queue_lengths(self) -> Dict[str, int]:
        """ã‚­ãƒ¥ãƒ¼ã®é•·ã•å–å¾—"""
        try:
            # Celeryã‚¤ãƒ³ã‚¹ãƒšã‚¯ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã‚­ãƒ¥ãƒ¼ã®çŠ¶æ…‹ã‚’å–å¾—
            from celery import current_app
            inspect = current_app.control.inspect()
            
            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ¯ãƒ¼ã‚«ãƒ¼ã‹ã‚‰æƒ…å ±ã‚’å–å¾—
            active_queues = inspect.active_queues()
            
            if not active_queues:
                return {}
            
            # Redis ã‹ã‚‰ç›´æ¥ã‚­ãƒ¥ãƒ¼ã®é•·ã•ã‚’å–å¾—
            import redis
            redis_client = redis.Redis.from_url(self.celery_app.conf.broker_url)
            
            queue_lengths = {}
            for queue_config in self.celery_app.conf.task_queues:
                queue_name = queue_config.name
                queue_key = f"celery:{queue_name}"
                length = redis_client.llen(queue_key)
                queue_lengths[queue_name] = length
            
            return queue_lengths
            
        except Exception as e:
            logger.error(f"Failed to get queue lengths: {e}")
            return {}
    
    def start_worker(self, worker_name: str = None) -> str:
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ã®é–‹å§‹"""
        if worker_name is None:
            worker_name = f"worker_{int(time.time())}"
        
        # ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã®èµ·å‹•
        cmd = [
            'celery', 'worker',
            '-A', 'tasks',
            '--hostname', f'{worker_name}@%h',
            '--loglevel', 'INFO',
            '--without-gossip',
            '--without-mingle',
            '--without-heartbeat'
        ]
        
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        
        self.worker_processes[worker_name] = process
        logger.info(f"Started worker: {worker_name} (PID: {process.pid})")
        
        return worker_name
    
    def stop_worker(self, worker_name: str = None):
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ã®åœæ­¢"""
        if worker_name is None:
            # æœ€ã‚‚å¤ã„ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’é¸æŠ
            active_workers = {
                name: proc for name, proc in self.worker_processes.items()
                if proc.poll() is None
            }
            
            if active_workers:
                worker_name = min(active_workers.keys())
        
        if worker_name in self.worker_processes:
            process = self.worker_processes[worker_name]
            
            if process.poll() is None:
                # ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã‚’è©¦è¡Œ
                process.terminate()
                
                try:
                    process.wait(timeout=30)
                except subprocess.TimeoutExpired:
                    # å¼·åˆ¶çµ‚äº†
                    process.kill()
                    process.wait()
                
                logger.info(f"Stopped worker: {worker_name}")
            
            del self.worker_processes[worker_name]
    
    def stop_all_workers(self):
        """å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã®åœæ­¢"""
        for worker_name in list(self.worker_processes.keys()):
            self.stop_worker(worker_name)
    
    def _cleanup_dead_workers(self):
        """æ­»ã‚“ã ãƒ¯ãƒ¼ã‚«ãƒ¼ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        dead_workers = []
        
        for worker_name, process in self.worker_processes.items():
            if process.poll() is not None:
                dead_workers.append(worker_name)
        
        for worker_name in dead_workers:
            logger.warning(f"Cleaning up dead worker: {worker_name}")
            del self.worker_processes[worker_name]
    
    def get_worker_stats(self) -> Dict[str, Any]:
        """ãƒ¯ãƒ¼ã‚«ãƒ¼çµ±è¨ˆã®å–å¾—"""
        active_workers = len([p for p in self.worker_processes.values() if p.poll() is None])
        dead_workers = len([p for p in self.worker_processes.values() if p.poll() is not None])
        
        return {
            "total_workers": len(self.worker_processes),
            "active_workers": active_workers,
            "dead_workers": dead_workers,
            "cpu_percent": psutil.cpu_percent(),
            "memory_percent": psutil.virtual_memory().percent,
            "queue_lengths": self._get_queue_lengths()
        }

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
worker_manager = DynamicWorkerManager(app)

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã«ãƒ¯ãƒ¼ã‚«ãƒ¼ç®¡ç†ã‚’é–‹å§‹
def start_worker_management():
    """ãƒ¯ãƒ¼ã‚«ãƒ¼ç®¡ç†ã®é–‹å§‹"""
    worker_manager.start_monitoring()

def stop_worker_management():
    """ãƒ¯ãƒ¼ã‚«ãƒ¼ç®¡ç†ã®åœæ­¢"""
    worker_manager.stop_monitoring()
```

## ä½¿ç”¨ä¾‹

```python
# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•
if __name__ == "__main__":
    import signal
    import sys
    
    # ãƒ¯ãƒ¼ã‚«ãƒ¼ç®¡ç†ã®é–‹å§‹
    start_worker_management()
    
    # ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã®è¨­å®š
    def signal_handler(sig, frame):
        print("Gracefully shutting down...")
        stop_worker_management()
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        stop_worker_management()
```

## è¨­å®šã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### ç’°å¢ƒå¤‰æ•°è¨­å®š

```bash
# Celeryè¨­å®š
export CELERY_BROKER_URL="redis://localhost:6379/0"
export CELERY_RESULT_BACKEND="redis://localhost:6379/0"

# ãƒ¯ãƒ¼ã‚«ãƒ¼ç®¡ç†è¨­å®š
export MAX_WORKERS=10
export MIN_WORKERS=2
export CPU_THRESHOLD=80.0
export MEMORY_THRESHOLD=85.0
export QUEUE_THRESHOLD=100

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š
export DB_HOST="localhost"
export DB_NAME="app_db"
export DB_USER="app_user"
export DB_PASSWORD="secure_password"
```

### Dockerã‚³ãƒ³ãƒ†ãƒŠã§ã®ä½¿ç”¨

```yaml
version: '3.8'
services:
  celery-worker:
    build: .
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - MAX_WORKERS=5
      - MIN_WORKERS=1
    volumes:
      - ./app:/app
    depends_on:
      - redis
      - postgres
```