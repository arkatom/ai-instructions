# Error Handling & Retry Strategies

> ğŸ¯ **ç›®çš„**: Celeryã®ä¼æ¥­ãƒ¬ãƒ™ãƒ«ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨é«˜åº¦ãªãƒªãƒˆãƒ©ã‚¤æˆ¦ç•¥
> 
> ğŸ“Š **å¯¾è±¡**: ã‚¨ãƒ©ãƒ¼åˆ†é¡ã€ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ã€é€šçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã€å›å¾©å¯èƒ½ã‚¿ã‚¹ã‚¯
> 
> âš¡ **ç‰¹å¾´**: åŒ…æ‹¬çš„ã‚¨ãƒ©ãƒ¼å‡¦ç†ã€æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã€éšœå®³å¾©æ—§ã€ç›£è¦–é€£æº

## åŒ…æ‹¬çš„ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 

### ã‚¨ãƒ©ãƒ¼åˆ†é¡ã¨å‡¦ç†ã‚¯ãƒ©ã‚¹

```python
from celery.exceptions import Retry, WorkerLostError, SoftTimeLimitExceeded
import traceback
from functools import wraps
from typing import Type, List, Callable, Dict, Any
from dataclasses import dataclass
from enum import Enum
import random
import time
import json
import hashlib

class ErrorSeverity(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

@dataclass
class RetryPolicy:
    """ãƒªãƒˆãƒ©ã‚¤ãƒãƒªã‚·ãƒ¼è¨­å®š"""
    max_retries: int = 3
    base_delay: float = 1.0
    max_delay: float = 300.0
    backoff_factor: float = 2.0
    jitter: bool = True
    exceptions: List[Type[Exception]] = None

class CeleryErrorHandler:
    """ä¼æ¥­ãƒ¬ãƒ™ãƒ«Celeryã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    
    def __init__(self):
        self.error_callbacks = {}
        self.retry_policies = {}
        self.circuit_breakers = {}
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒªãƒˆãƒ©ã‚¤ãƒãƒªã‚·ãƒ¼
        self.default_retry_policy = RetryPolicy(
            max_retries=3,
            base_delay=1.0,
            backoff_factor=2.0,
            exceptions=[ConnectionError, TimeoutError]
        )
    
    def register_error_callback(self, exception_type: Type[Exception], callback: Callable):
        """ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®ç™»éŒ²"""
        self.error_callbacks[exception_type] = callback
    
    def register_retry_policy(self, task_name: str, policy: RetryPolicy):
        """ã‚¿ã‚¹ã‚¯åˆ¥ãƒªãƒˆãƒ©ã‚¤ãƒãƒªã‚·ãƒ¼ã®ç™»éŒ²"""
        self.retry_policies[task_name] = policy
    
    def handle_task_error(self, task, exc: Exception, task_id: str, args: tuple, kwargs: dict):
        """ã‚¿ã‚¹ã‚¯ã‚¨ãƒ©ãƒ¼ã®åŒ…æ‹¬çš„å‡¦ç†"""
        
        # ã‚¨ãƒ©ãƒ¼ã®åˆ†é¡
        severity = self._classify_error(exc)
        
        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®è¨˜éŒ²
        self._log_error(task, exc, task_id, args, kwargs, severity)
        
        # ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®å®Ÿè¡Œ
        self._execute_error_callbacks(exc, task, task_id)
        
        # ãƒªãƒˆãƒ©ã‚¤ã®åˆ¤å®š
        return self._should_retry(task, exc)
    
    def _classify_error(self, exc: Exception) -> ErrorSeverity:
        """ã‚¨ãƒ©ãƒ¼ã®è‡ªå‹•åˆ†é¡"""
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é–¢é€£ã‚¨ãƒ©ãƒ¼
        if isinstance(exc, (ConnectionError, TimeoutError)):
            return ErrorSeverity.MEDIUM
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–¢é€£ã‚¨ãƒ©ãƒ¼
        if 'database' in str(type(exc)).lower() or 'sql' in str(type(exc)).lower():
            return ErrorSeverity.HIGH
        
        # ãƒ¡ãƒ¢ãƒªé–¢é€£ã‚¨ãƒ©ãƒ¼
        if isinstance(exc, MemoryError):
            return ErrorSeverity.CRITICAL
        
        # ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢é€£ã‚¨ãƒ©ãƒ¼
        if isinstance(exc, (WorkerLostError, SoftTimeLimitExceeded)):
            return ErrorSeverity.HIGH
        
        return ErrorSeverity.LOW
    
    def _log_error(self, task, exc: Exception, task_id: str, args: tuple, kwargs: dict, severity: ErrorSeverity):
        """æ§‹é€ åŒ–ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®è¨˜éŒ²"""
        
        error_data = {
            'task_id': task_id,
            'task_name': task.name,
            'exception_type': type(exc).__name__,
            'exception_message': str(exc),
            'severity': severity.value,
            'args': args,
            'kwargs': kwargs,
            'traceback': traceback.format_exc(),
            'timestamp': time.time()
        }
        
        # Redis ã«ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ä¿å­˜
        redis_client = redis.Redis.from_url(app.conf.result_backend)
        redis_client.lpush('celery_errors', json.dumps(error_data))
        redis_client.ltrim('celery_errors', 0, 9999)
        
        # é‡å¤§ã‚¨ãƒ©ãƒ¼ã®å³åº§é€šçŸ¥
        if severity in [ErrorSeverity.HIGH, ErrorSeverity.CRITICAL]:
            self._send_critical_error_notification(error_data)
```

## é«˜åº¦ãªãƒªãƒˆãƒ©ã‚¤æˆ¦ç•¥

### å …ç‰¢ãªã‚¿ã‚¹ã‚¯ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿

```python
def robust_task(retry_policy: RetryPolicy = None, error_callback: Callable = None):
    """å …ç‰¢ãªã‚¿ã‚¹ã‚¯ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    
    def decorator(func):
        @wraps(func)
        def wrapper(self, *args, **kwargs):
            
            # ãƒªãƒˆãƒ©ã‚¤ãƒãƒªã‚·ãƒ¼ã®è¨­å®š
            policy = retry_policy or error_handler.retry_policies.get(func.__name__, error_handler.default_retry_policy)
            
            try:
                return func(self, *args, **kwargs)
                
            except Exception as exc:
                
                # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
                should_retry = error_handler.handle_task_error(self, exc, self.request.id, args, kwargs)
                
                # ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®å®Ÿè¡Œ
                if error_callback:
                    try:
                        error_callback(exc, self, self.request.id)
                    except Exception:
                        pass
                
                # ãƒªãƒˆãƒ©ã‚¤ã®å®Ÿè¡Œ
                if should_retry and self.request.retries < policy.max_retries:
                    
                    # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã®è¨ˆç®—
                    delay = policy.base_delay * (policy.backoff_factor ** self.request.retries)
                    delay = min(delay, policy.max_delay)
                    
                    # ã‚¸ãƒƒã‚¿ãƒ¼ã®è¿½åŠ 
                    if policy.jitter:
                        jitter = random.uniform(0.1, 0.3) * delay
                        delay += jitter
                    
                    logger.warning(f"Task {self.name} failed, retrying in {delay:.2f}s (attempt {self.request.retries + 1}/{policy.max_retries})")
                    
                    raise self.retry(countdown=delay, exc=exc)
                
                # æœ€çµ‚çš„ãªå¤±æ•—
                logger.error(f"Task {self.name} failed permanently after {self.request.retries} retries")
                raise exc
        
        return wrapper
    return decorator

@app.task(bind=True, base=BaseTask)
@robust_task(
    retry_policy=RetryPolicy(
        max_retries=5,
        base_delay=2.0,
        max_delay=120.0,
        exceptions=[ConnectionError, TimeoutError, requests.RequestException]
    )
)
def resilient_api_call(self, url: str, method: str = 'GET', **kwargs):
    """å›å¾©å¯èƒ½ãªAPIå‘¼ã³å‡ºã—ã‚¿ã‚¹ã‚¯"""
    
    import requests
    
    try:
        self.update_state(
            state='PROGRESS',
            meta={'status': f'Making {method} request to {url}'}
        )
        
        # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å®Ÿè¡Œ
        response = requests.request(method, url, timeout=30, **kwargs)
        response.raise_for_status()
        
        return {
            'status_code': response.status_code,
            'headers': dict(response.headers),
            'data': response.json() if response.headers.get('content-type', '').startswith('application/json') else response.text
        }
        
    except requests.exceptions.HTTPError as exc:
        if exc.response.status_code >= 500:
            # ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ã¯ãƒªãƒˆãƒ©ã‚¤
            raise exc
        else:
            # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼ã¯ãƒªãƒˆãƒ©ã‚¤ã—ãªã„
            logger.error(f"Client error for {url}: {exc}")
            raise exc
```

## ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³

### éšœå®³ã®é€£é–é˜²æ­¢ã‚·ã‚¹ãƒ†ãƒ 

```python
class CircuitBreaker:
    """ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼å®Ÿè£…"""
    
    def __init__(self, failure_threshold: int = 5, timeout: float = 60.0):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failure_count = 0
        self.last_failure_time = 0
        self.state = 'closed'  # closed, open, half_open
    
    def call(self, func: Callable, *args, **kwargs):
        """ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼çµŒç”±ã®é–¢æ•°å‘¼ã³å‡ºã—"""
        
        current_time = time.time()
        
        # ã‚ªãƒ¼ãƒ—ãƒ³çŠ¶æ…‹ã®ãƒã‚§ãƒƒã‚¯
        if self.state == 'open':
            if current_time - self.last_failure_time > self.timeout:
                self.state = 'half_open'
            else:
                raise Exception(f"Circuit breaker is open")
        
        try:
            result = func(*args, **kwargs)
            
            # æˆåŠŸæ™‚ã®å‡¦ç†
            if self.state == 'half_open':
                self.state = 'closed'
                self.failure_count = 0
            
            return result
            
        except Exception as exc:
            self.failure_count += 1
            self.last_failure_time = current_time
            
            if self.failure_count >= self.failure_threshold:
                self.state = 'open'
                logger.warning(f"Circuit breaker opened due to {self.failure_count} failures")
            
            raise exc

# ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ä»˜ãã‚¿ã‚¹ã‚¯
external_api_circuit_breaker = CircuitBreaker(failure_threshold=3, timeout=30.0)

@app.task(bind=True, base=BaseTask)
def external_api_with_circuit_breaker(self, api_endpoint: str, **kwargs):
    """ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ä»˜ãå¤–éƒ¨APIå‘¼ã³å‡ºã—"""
    
    def api_call():
        return resilient_api_call.apply_async(args=[api_endpoint], kwargs=kwargs).get()
    
    try:
        result = external_api_circuit_breaker.call(api_call)
        return result
        
    except Exception as exc:
        logger.error(f"External API call failed: {exc}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
        return {
            'status': 'fallback',
            'message': 'External service unavailable, using cached data',
            'data': get_cached_api_response(api_endpoint)
        }
```

## ã‚¨ãƒ©ãƒ¼é€šçŸ¥ã¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯

### é‡å¤§ã‚¨ãƒ©ãƒ¼ã®å³åº§é€šçŸ¥ã‚·ã‚¹ãƒ†ãƒ 

```python
@app.task(base=BaseTask, queue='priority')
def send_error_notification(error_type: str, error_data: Dict[str, Any], recipients: List[str]):
    """ã‚¨ãƒ©ãƒ¼é€šçŸ¥ã®é€ä¿¡"""
    
    try:
        if error_type == "critical_task_failure":
            subject = f"Critical Task Failure: {error_data['task_name']}"
            body = f"""
            Critical task failure detected:
            
            Task: {error_data['task_name']}
            Task ID: {error_data['task_id']}
            Exception: {error_data['exception_type']} - {error_data['exception_message']}
            Severity: {error_data['severity']}
            Time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(error_data['timestamp']))}
            
            Traceback:
            {error_data['traceback']}
            """
        
        # é€šçŸ¥ã®é€ä¿¡
        for recipient in recipients:
            send_email_notification(recipient, subject, body)
        
        return {'sent_to': recipients, 'status': 'success'}
        
    except Exception as exc:
        logger.error(f"Failed to send error notification: {exc}")
        raise exc

def database_error_callback(exc: Exception, task, task_id: str):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    
    try:
        global db_pool
        if db_pool:
            db_pool.closeall()
        init_worker()
        logger.info("Database connection pool reset due to error")
        
    except Exception as reset_exc:
        logger.error(f"Failed to reset database connection pool: {reset_exc}")

def memory_error_callback(exc: Exception, task, task_id: str):
    """ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    
    import gc
    gc.collect()
    logger.critical(f"Memory error in task {task_id}, worker restart recommended")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š
error_handler = CeleryErrorHandler()

# ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®ç™»éŒ²
error_handler.register_error_callback(psycopg2.Error, database_error_callback)
error_handler.register_error_callback(MemoryError, memory_error_callback)

# ã‚«ã‚¹ã‚¿ãƒ ãƒªãƒˆãƒ©ã‚¤ãƒãƒªã‚·ãƒ¼ã®è¨­å®šä¾‹
api_retry_policy = RetryPolicy(
    max_retries=5,
    base_delay=3.0,
    max_delay=300.0,
    backoff_factor=2.5,
    jitter=True,
    exceptions=[ConnectionError, TimeoutError, requests.RequestException]
)

error_handler.register_retry_policy('resilient_api_call', api_retry_policy)
```

## ä½¿ç”¨ä¾‹ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### å®Ÿé‹ç”¨ã§ã®æ´»ç”¨æ–¹æ³•

```python
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œã®å›å¾©å¯èƒ½ã‚¿ã‚¹ã‚¯
@app.task(bind=True, base=BaseTask)
@robust_task(
    retry_policy=RetryPolicy(
        max_retries=3,
        base_delay=5.0,
        exceptions=[psycopg2.OperationalError, psycopg2.InterfaceError]
    )
)
def resilient_database_operation(self, operation: str, query: str, parameters: List[Any] = None):
    """å›å¾©å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œ"""
    
    max_connection_attempts = 3
    
    for attempt in range(max_connection_attempts):
        try:
            with get_db_connection() as conn:
                cursor = conn.cursor()
                
                if operation == 'select':
                    cursor.execute(query, parameters)
                    result = cursor.fetchall()
                    return {'rows': result, 'rowcount': cursor.rowcount}
                
                elif operation in ['insert', 'update', 'delete']:
                    cursor.execute(query, parameters)
                    conn.commit()
                    return {'rowcount': cursor.rowcount}
                    
        except (psycopg2.OperationalError, psycopg2.InterfaceError) as exc:
            if attempt < max_connection_attempts - 1:
                wait_time = (attempt + 1) * 2
                logger.warning(f"Database connection failed, retrying in {wait_time}s: {exc}")
                time.sleep(wait_time)
                continue
            else:
                raise exc

def get_cached_api_response(api_endpoint: str) -> Dict[str, Any]:
    """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—"""
    
    redis_client = redis.Redis.from_url(app.conf.result_backend)
    cache_key = f"api_cache:{hashlib.md5(api_endpoint.encode()).hexdigest()}"
    
    cached_data = redis_client.get(cache_key)
    if cached_data:
        return json.loads(cached_data)
    
    return {'message': 'No cached data available'}
```