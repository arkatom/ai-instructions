# Task Patterns & Best Practices

> ğŸ¯ **ç›®çš„**: Celeryã®é«˜åº¦ãªã‚¿ã‚¹ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨å®Ÿè£…ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
> 
> ğŸ“Š **å¯¾è±¡**: ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚¹ã‚¯å®Ÿè£…ã€ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã€é€²æ—è¿½è·¡ã€æ¡ä»¶ä»˜ãå®Ÿè¡Œ
> 
> âš¡ **ç‰¹å¾´**: ã¹ãç­‰æ€§ã€ãƒªãƒˆãƒ©ã‚¤æˆ¦ç•¥ã€ãƒãƒƒãƒå‡¦ç†ã€å‹•çš„ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ

## é«˜åº¦ãªã‚¿ã‚¹ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³

### ã¹ãç­‰æ€§ã¨ãƒªãƒˆãƒ©ã‚¤ã®ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿

```python
from celery import Task, group, chain, chord, chunks
from functools import wraps
import time
import json
from typing import Any, Dict, List, Optional, Callable
import hashlib
import pickle

# ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚¹ã‚¯ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
def idempotent_task(func):
    """ã¹ãç­‰ã‚¿ã‚¹ã‚¯ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        # ã‚¿ã‚¹ã‚¯IDã®ç”Ÿæˆï¼ˆå¼•æ•°ãƒ™ãƒ¼ã‚¹ï¼‰
        task_signature = f"{func.__name__}:{args}:{sorted(kwargs.items())}"
        task_hash = hashlib.md5(task_signature.encode()).hexdigest()
        
        # Redis ã§ã¹ãç­‰æ€§ãƒã‚§ãƒƒã‚¯
        redis_client = redis.Redis.from_url(app.conf.result_backend)
        result_key = f"idempotent:{task_hash}"
        
        cached_result = redis_client.get(result_key)
        if cached_result:
            return pickle.loads(cached_result)
        
        # ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
        result = func(*args, **kwargs)
        
        # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆ24æ™‚é–“ï¼‰
        redis_client.setex(result_key, 86400, pickle.dumps(result))
        
        return result
    
    return wrapper

def retry_with_backoff(max_retries=3, base_delay=1, max_delay=60):
    """æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ãƒªãƒˆãƒ©ã‚¤ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func):
        @wraps(func)
        def wrapper(self, *args, **kwargs):
            for attempt in range(max_retries + 1):
                try:
                    return func(self, *args, **kwargs)
                except Exception as exc:
                    if attempt == max_retries:
                        raise exc
                    
                    # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•è¨ˆç®—
                    delay = min(base_delay * (2 ** attempt), max_delay)
                    
                    # ã‚¸ãƒƒã‚¿ãƒ¼ã®è¿½åŠ 
                    import random
                    jitter = random.uniform(0.1, 0.3) * delay
                    final_delay = delay + jitter
                    
                    logger.warning(f"Task failed, retrying in {final_delay:.2f}s (attempt {attempt + 1}/{max_retries})")
                    
                    # ãƒªãƒˆãƒ©ã‚¤
                    raise self.retry(countdown=final_delay, exc=exc)
        
        return wrapper
    return decorator
```

### é€²æ—è¿½è·¡ä»˜ãå¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†

```python
# é«˜åº¦ãªã‚¿ã‚¹ã‚¯å®Ÿè£…
@app.task(bind=True, base=BaseTask)
@idempotent_task
@retry_with_backoff(max_retries=3)
def process_large_dataset(self, dataset_id: int, processing_options: Dict[str, Any]):
    """å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†ã‚¿ã‚¹ã‚¯"""
    
    # é€²æ—è¿½è·¡ã®åˆæœŸåŒ–
    self.update_state(
        state='PROGRESS',
        meta={'current': 0, 'total': 100, 'status': 'Starting processing...'}
    )
    
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å–å¾—
            cursor.execute("SELECT data FROM datasets WHERE id = %s", (dataset_id,))
            dataset_row = cursor.fetchone()
            
            if not dataset_row:
                raise ValueError(f"Dataset {dataset_id} not found")
            
            dataset = json.loads(dataset_row[0])
            total_items = len(dataset)
            
            processed_items = []
            
            for i, item in enumerate(dataset):
                # ã‚¢ã‚¤ãƒ†ãƒ ã®å‡¦ç†
                processed_item = process_single_item(item, processing_options)
                processed_items.append(processed_item)
                
                # é€²æ—æ›´æ–°
                if i % 10 == 0:  # 10ã‚¢ã‚¤ãƒ†ãƒ ã”ã¨ã«æ›´æ–°
                    progress = int((i + 1) / total_items * 100)
                    self.update_state(
                        state='PROGRESS',
                        meta={
                            'current': i + 1,
                            'total': total_items,
                            'status': f'Processed {i + 1}/{total_items} items',
                            'progress': progress
                        }
                    )
            
            # çµæœã®ä¿å­˜
            result_data = {
                'dataset_id': dataset_id,
                'processed_count': len(processed_items),
                'processing_options': processing_options,
                'timestamp': time.time()
            }
            
            cursor.execute(
                "INSERT INTO processing_results (dataset_id, result_data) VALUES (%s, %s)",
                (dataset_id, json.dumps(result_data))
            )
            conn.commit()
            
            return result_data
            
    except Exception as exc:
        # ã‚¨ãƒ©ãƒ¼çŠ¶æ…‹ã®æ›´æ–°
        self.update_state(
            state='FAILURE',
            meta={'error': str(exc), 'traceback': traceback.format_exc()}
        )
        raise exc

def process_single_item(item: Dict[str, Any], options: Dict[str, Any]) -> Dict[str, Any]:
    """å˜ä¸€ã‚¢ã‚¤ãƒ†ãƒ ã®å‡¦ç†"""
    # å®Ÿéš›ã®å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯
    processed_item = item.copy()
    
    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«åŸºã¥ãå‡¦ç†
    if options.get('normalize'):
        processed_item = normalize_item(processed_item)
    
    if options.get('validate'):
        validate_item(processed_item)
    
    return processed_item
```

### é€šçŸ¥ã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¿ã‚¹ã‚¯

```python
@app.task(bind=True, base=BaseTask)
def send_notification_email(self, recipient: str, subject: str, body: str, template_id: str = None):
    """é€šçŸ¥ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚¿ã‚¹ã‚¯"""
    
    try:
        # ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒ“ã‚¹ã®åˆæœŸåŒ–
        email_service = EmailService()
        
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é©ç”¨
        if template_id:
            body = email_service.apply_template(template_id, body)
        
        # ãƒ¡ãƒ¼ãƒ«é€ä¿¡
        success = email_service.send_email(
            to=recipient,
            subject=subject,
            body=body
        )
        
        if not success:
            raise Exception("Failed to send email")
        
        # é€ä¿¡å±¥æ­´ã®è¨˜éŒ²
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(
                "INSERT INTO email_logs (recipient, subject, sent_at, status) VALUES (%s, %s, %s, %s)",
                (recipient, subject, time.time(), 'sent')
            )
            conn.commit()
        
        return {"status": "sent", "recipient": recipient}
        
    except Exception as exc:
        # å¤±æ•—ãƒ­ã‚°ã®è¨˜éŒ²
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(
                "INSERT INTO email_logs (recipient, subject, sent_at, status, error) VALUES (%s, %s, %s, %s, %s)",
                (recipient, subject, time.time(), 'failed', str(exc))
            )
            conn.commit()
        
        raise exc

@app.task(bind=True, base=BaseTask, queue='cpu_intensive')
def generate_report(self, report_type: str, parameters: Dict[str, Any]):
    """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¿ã‚¹ã‚¯"""
    
    self.update_state(
        state='PROGRESS',
        meta={'status': 'Initializing report generation...'}
    )
    
    try:
        report_generator = ReportGenerator(report_type)
        
        # ãƒ‡ãƒ¼ã‚¿åé›†ãƒ•ã‚§ãƒ¼ã‚º
        self.update_state(
            state='PROGRESS',
            meta={'status': 'Collecting data...', 'progress': 25}
        )
        
        data = report_generator.collect_data(parameters)
        
        # åˆ†æãƒ•ã‚§ãƒ¼ã‚º
        self.update_state(
            state='PROGRESS',
            meta={'status': 'Analyzing data...', 'progress': 50}
        )
        
        analysis = report_generator.analyze_data(data)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚º
        self.update_state(
            state='PROGRESS',
            meta={'status': 'Generating report...', 'progress': 75}
        )
        
        report_content = report_generator.generate_report(analysis)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        self.update_state(
            state='PROGRESS',
            meta={'status': 'Saving report...', 'progress': 90}
        )
        
        file_path = report_generator.save_report(report_content, parameters)
        
        return {
            "report_type": report_type,
            "file_path": file_path,
            "parameters": parameters,
            "generated_at": time.time()
        }
        
    except Exception as exc:
        logger.error(f"Report generation failed: {exc}")
        raise exc
```

## ãƒãƒƒãƒå‡¦ç†ãƒ‘ã‚¿ãƒ¼ãƒ³

### ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒãƒƒãƒå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 

```python
# ãƒãƒƒãƒå‡¦ç†ãƒ‘ã‚¿ãƒ¼ãƒ³
@app.task(bind=True, base=BaseTask)
def batch_process_users(self, user_ids: List[int], operation: str, batch_size: int = 100):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒãƒƒãƒå‡¦ç†ã‚¿ã‚¹ã‚¯"""
    
    total_users = len(user_ids)
    processed_count = 0
    failed_count = 0
    
    # ãƒãƒƒãƒã”ã¨ã®å‡¦ç†
    for i in range(0, total_users, batch_size):
        batch = user_ids[i:i + batch_size]
        
        # ãƒãƒƒãƒå‡¦ç†ã®å®Ÿè¡Œ
        batch_result = process_user_batch.delay(batch, operation)
        
        try:
            result = batch_result.get(timeout=300)  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            processed_count += result['processed']
            failed_count += result['failed']
            
        except Exception as exc:
            logger.error(f"Batch processing failed for batch {i//batch_size + 1}: {exc}")
            failed_count += len(batch)
        
        # é€²æ—æ›´æ–°
        progress = int((i + len(batch)) / total_users * 100)
        self.update_state(
            state='PROGRESS',
            meta={
                'current': i + len(batch),
                'total': total_users,
                'processed': processed_count,
                'failed': failed_count,
                'progress': progress
            }
        )
    
    return {
        'total_users': total_users,
        'processed': processed_count,
        'failed': failed_count,
        'operation': operation
    }

@app.task(base=BaseTask)
def process_user_batch(user_ids: List[int], operation: str):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒãƒƒãƒã®å‡¦ç†"""
    
    processed = 0
    failed = 0
    
    with get_db_connection() as conn:
        cursor = conn.cursor()
        
        for user_id in user_ids:
            try:
                if operation == 'activate':
                    cursor.execute("UPDATE users SET is_active = TRUE WHERE id = %s", (user_id,))
                elif operation == 'deactivate':
                    cursor.execute("UPDATE users SET is_active = FALSE WHERE id = %s", (user_id,))
                elif operation == 'delete':
                    cursor.execute("DELETE FROM users WHERE id = %s", (user_id,))
                
                processed += 1
                
            except Exception as exc:
                logger.error(f"Failed to process user {user_id}: {exc}")
                failed += 1
        
        conn.commit()
    
    return {'processed': processed, 'failed': failed}
```

## æ¡ä»¶ä»˜ãã‚¿ã‚¹ã‚¯å®Ÿè¡Œ

### å‹•çš„æ¡ä»¶è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 

```python
# æ¡ä»¶ä»˜ãã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
@app.task(bind=True, base=BaseTask)
def conditional_processing(self, data: Dict[str, Any], conditions: Dict[str, Any]):
    """æ¡ä»¶ä»˜ãå‡¦ç†ã‚¿ã‚¹ã‚¯"""
    
    results = []
    
    # æ¡ä»¶è©•ä¾¡
    for condition_name, condition_config in conditions.items():
        condition_type = condition_config['type']
        condition_params = condition_config['params']
        
        if evaluate_condition(data, condition_type, condition_params):
            # æ¡ä»¶ãŒæº€ãŸã•ã‚ŒãŸå ´åˆã®ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
            task_name = condition_config['task']
            task_params = condition_config.get('task_params', {})
            
            # å‹•çš„ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
            task = app.tasks[task_name]
            result = task.delay(**task_params)
            
            results.append({
                'condition': condition_name,
                'task': task_name,
                'task_id': result.id,
                'status': 'scheduled'
            })
    
    return results

def evaluate_condition(data: Dict[str, Any], condition_type: str, params: Dict[str, Any]) -> bool:
    """æ¡ä»¶è©•ä¾¡"""
    
    if condition_type == 'value_equals':
        field = params['field']
        expected_value = params['value']
        return data.get(field) == expected_value
    
    elif condition_type == 'value_greater_than':
        field = params['field']
        threshold = params['threshold']
        return data.get(field, 0) > threshold
    
    elif condition_type == 'field_exists':
        field = params['field']
        return field in data
    
    elif condition_type == 'custom_function':
        function_name = params['function']
        # ã‚«ã‚¹ã‚¿ãƒ é–¢æ•°ã®å®Ÿè¡Œ
        function = globals().get(function_name)
        if function:
            return function(data, params)
    
    return False
```

## ä½¿ç”¨ä¾‹ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### ã‚¿ã‚¹ã‚¯å®šç¾©ã®ä¾‹

```python
# ä½¿ç”¨ä¾‹
def example_task_patterns():
    """ã‚¿ã‚¹ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä½¿ç”¨ä¾‹"""
    
    # ã¹ãç­‰ã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œ
    result1 = process_large_dataset.delay(
        dataset_id=123,
        processing_options={
            'normalize': True,
            'validate': True,
            'quality_threshold': 0.95
        }
    )
    
    # ãƒãƒƒãƒå‡¦ç†ã®å®Ÿè¡Œ
    user_ids = list(range(1, 1001))  # 1000ãƒ¦ãƒ¼ã‚¶ãƒ¼
    result2 = batch_process_users.delay(
        user_ids=user_ids,
        operation='activate',
        batch_size=50
    )
    
    # æ¡ä»¶ä»˜ãå‡¦ç†ã®å®Ÿè¡Œ
    conditions = {
        'high_priority': {
            'type': 'value_greater_than',
            'params': {'field': 'priority_score', 'threshold': 80},
            'task': 'high_priority_processing',
            'task_params': {'priority_level': 'urgent'}
        },
        'data_validation_required': {
            'type': 'field_exists',
            'params': {'field': 'validation_required'},
            'task': 'validate_data_quality',
            'task_params': {'strict_mode': True}
        }
    }
    
    result3 = conditional_processing.delay(
        data={'priority_score': 85, 'validation_required': True},
        conditions=conditions
    )
    
    # çµæœã®å–å¾—
    print(f"Dataset processing: {result1.get()}")
    print(f"Batch processing: {result2.get()}")
    print(f"Conditional processing: {result3.get()}")
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®ãƒ’ãƒ³ãƒˆ

```python
# ã‚¿ã‚¹ã‚¯æœ€é©åŒ–ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

# 1. é©åˆ‡ãªã‚­ãƒ¥ãƒ¼ã®ä½¿ç”¨
@app.task(queue='cpu_intensive')  # CPUé›†ç´„çš„ã‚¿ã‚¹ã‚¯
def cpu_heavy_task():
    pass

@app.task(queue='io_intensive')   # I/Oé›†ç´„çš„ã‚¿ã‚¹ã‚¯
def io_heavy_task():
    pass

# 2. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®è€ƒæ…®
@app.task(bind=True)
def memory_efficient_task(self, large_data_id):
    # å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥æ¸¡ã•ãšã€IDã§å‚ç…§
    with get_db_connection() as conn:
        data = fetch_data_by_id(conn, large_data_id)
        process_data(data)

# 3. é©åˆ‡ãªã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
@app.task(soft_time_limit=300, time_limit=360)  # 5åˆ†ã‚½ãƒ•ãƒˆåˆ¶é™ã€6åˆ†ãƒãƒ¼ãƒ‰åˆ¶é™
def time_limited_task():
    pass

# 4. ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†
@app.task(bind=True)
def resource_managed_task(self):
    try:
        # ãƒªã‚½ãƒ¼ã‚¹ã®å–å¾—
        resource = acquire_limited_resource()
        
        # å‡¦ç†å®Ÿè¡Œ
        result = process_with_resource(resource)
        
        return result
    finally:
        # ãƒªã‚½ãƒ¼ã‚¹ã®ç¢ºå®Ÿãªè§£æ”¾
        if 'resource' in locals():
            release_resource(resource)
```