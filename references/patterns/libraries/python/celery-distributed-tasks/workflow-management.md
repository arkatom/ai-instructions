# Workflow Management

> ğŸ¯ **ç›®çš„**: Celeryã‚’ä½¿ç”¨ã—ãŸè¤‡é›‘ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…
> 
> ğŸ“Š **å¯¾è±¡**: ä¾å­˜é–¢ä¿‚ã‚’æŒã¤ã‚¿ã‚¹ã‚¯ãƒã‚§ãƒ¼ãƒ³ã€æ¡ä»¶ä»˜ãå®Ÿè¡Œã€ä¸¦åˆ—å‡¦ç†ã®çµ±åˆç®¡ç†
> 
> âš¡ **ç‰¹å¾´**: å‹•çš„ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã€ä¾å­˜é–¢ä¿‚è§£æã€å®Ÿè¡ŒçŠ¶æ…‹ç®¡ç†ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

## ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¨ãƒ³ã‚¸ãƒ³ã®å®Ÿè£…

### åŸºæœ¬ã‚¯ãƒ©ã‚¹å®šç¾©

```python
from celery import signature, chain, group, chord
from celery.result import GroupResult
import json
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from enum import Enum

class WorkflowStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILURE = "failure"
    CANCELLED = "cancelled"

@dataclass
class WorkflowStep:
    """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¹ãƒ†ãƒƒãƒ—"""
    name: str
    task_name: str
    parameters: Dict[str, Any]
    depends_on: List[str] = None
    retry_count: int = 3
    timeout: int = 300
    queue: str = 'default'
    
    def __post_init__(self):
        """åˆæœŸåŒ–å¾Œã®å‡¦ç†"""
        if self.depends_on is None:
            self.depends_on = []
```

### ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¨ãƒ³ã‚¸ãƒ³

```python
class WorkflowEngine:
    """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.workflows = {}
        self.executions = {}
        self.execution_history = []
    
    def register_workflow(self, workflow_id: str, steps: List[WorkflowStep]):
        """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ç™»éŒ²"""
        # ä¾å­˜é–¢ä¿‚ã®æ¤œè¨¼
        self._validate_workflow(steps)
        self.workflows[workflow_id] = steps
        print(f"Workflow '{workflow_id}' registered with {len(steps)} steps")
    
    def _validate_workflow(self, steps: List[WorkflowStep]):
        """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ¤œè¨¼"""
        step_names = {step.name for step in steps}
        
        for step in steps:
            # ä¾å­˜é–¢ä¿‚ã®å­˜åœ¨ç¢ºèª
            for dependency in step.depends_on:
                if dependency not in step_names:
                    raise ValueError(f"Step '{step.name}' depends on unknown step '{dependency}'")
        
        # å¾ªç’°ä¾å­˜ã®æ¤œæŸ»
        self._check_circular_dependencies(steps)
    
    def _check_circular_dependencies(self, steps: List[WorkflowStep]):
        """å¾ªç’°ä¾å­˜ã®æ¤œæŸ»"""
        step_map = {step.name: step for step in steps}
        visited = set()
        recursion_stack = set()
        
        def has_cycle(step_name: str) -> bool:
            if step_name in recursion_stack:
                return True
            if step_name in visited:
                return False
            
            visited.add(step_name)
            recursion_stack.add(step_name)
            
            step = step_map[step_name]
            for dependency in step.depends_on:
                if has_cycle(dependency):
                    return True
            
            recursion_stack.remove(step_name)
            return False
        
        for step in steps:
            if step.name not in visited:
                if has_cycle(step.name):
                    raise ValueError(f"Circular dependency detected in workflow")
    
    def execute_workflow(self, workflow_id: str, input_data: Dict[str, Any]) -> str:
        """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ"""
        
        if workflow_id not in self.workflows:
            raise ValueError(f"Workflow {workflow_id} not found")
        
        steps = self.workflows[workflow_id]
        execution_id = f"{workflow_id}_{int(time.time())}"
        
        # ä¾å­˜é–¢ä¿‚ã®è§£æ
        execution_plan = self._create_execution_plan(steps)
        
        # å®Ÿè¡ŒçŠ¶æ…‹ã®åˆæœŸåŒ–
        self.executions[execution_id] = {
            'workflow_id': workflow_id,
            'status': WorkflowStatus.RUNNING,
            'steps': {},
            'input_data': input_data,
            'start_time': time.time(),
            'result': None,
            'error': None,
            'execution_plan': execution_plan
        }
        
        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®éåŒæœŸå®Ÿè¡Œ
        execute_workflow_async.delay(execution_id, execution_plan, input_data)
        
        return execution_id
    
    def _create_execution_plan(self, steps: List[WorkflowStep]) -> List[List[WorkflowStep]]:
        """å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã®ä½œæˆï¼ˆä¾å­˜é–¢ä¿‚ã‚’è€ƒæ…®ï¼‰"""
        
        # ä¾å­˜é–¢ä¿‚ã‚°ãƒ©ãƒ•ã®ä½œæˆ
        step_map = {step.name: step for step in steps}
        
        # ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ã‚½ãƒ¼ãƒˆ
        execution_levels = []
        remaining_steps = set(step.name for step in steps)
        
        while remaining_steps:
            # ä¾å­˜é–¢ä¿‚ã®ãªã„ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¦‹ã¤ã‘ã‚‹
            ready_steps = []
            
            for step_name in remaining_steps:
                step = step_map[step_name]
                dependencies = step.depends_on or []
                
                # å…¨ã¦ã®ä¾å­˜é–¢ä¿‚ãŒæº€ãŸã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if all(dep not in remaining_steps for dep in dependencies):
                    ready_steps.append(step)
            
            if not ready_steps:
                raise ValueError("Circular dependency detected in workflow")
            
            execution_levels.append(ready_steps)
            
            # å‡¦ç†æ¸ˆã¿ã‚¹ãƒ†ãƒƒãƒ—ã‚’å‰Šé™¤
            for step in ready_steps:
                remaining_steps.remove(step.name)
        
        return execution_levels
    
    def get_workflow_status(self, execution_id: str) -> Dict[str, Any]:
        """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çŠ¶æ…‹ã®å–å¾—"""
        if execution_id not in self.executions:
            raise ValueError(f"Execution {execution_id} not found")
        
        execution = self.executions[execution_id]
        
        # è©³ç´°ãªçŠ¶æ…‹æƒ…å ±ã‚’è¿½åŠ 
        status_info = execution.copy()
        
        if execution['status'] == WorkflowStatus.RUNNING:
            # å®Ÿè¡Œä¸­ã®é€²æ—æƒ…å ±
            total_steps = sum(len(level) for level in execution['execution_plan'])
            completed_steps = len([step for step in execution['steps'].values() if step['status'] == 'success'])
            status_info['progress'] = {
                'completed_steps': completed_steps,
                'total_steps': total_steps,
                'percentage': int((completed_steps / total_steps) * 100) if total_steps > 0 else 0
            }
        
        return status_info
    
    def cancel_workflow(self, execution_id: str):
        """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ã‚­ãƒ£ãƒ³ã‚»ãƒ«"""
        if execution_id in self.executions:
            self.executions[execution_id]['status'] = WorkflowStatus.CANCELLED
            self.executions[execution_id]['end_time'] = time.time()
            # å®Ÿè¡Œä¸­ã®ã‚¿ã‚¹ã‚¯ã®åœæ­¢å‡¦ç†
            print(f"Workflow {execution_id} cancelled")
    
    def get_execution_history(self) -> List[Dict[str, Any]]:
        """å®Ÿè¡Œå±¥æ­´ã®å–å¾—"""
        return [
            {
                'execution_id': exec_id,
                'workflow_id': exec_data['workflow_id'],
                'status': exec_data['status'],
                'start_time': exec_data['start_time'],
                'end_time': exec_data.get('end_time'),
                'duration': exec_data.get('end_time', time.time()) - exec_data['start_time']
            }
            for exec_id, exec_data in self.executions.items()
        ]

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¨ãƒ³ã‚¸ãƒ³
workflow_engine = WorkflowEngine()
```

## ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ 

### éåŒæœŸå®Ÿè¡Œã‚¿ã‚¹ã‚¯

```python
@app.task(bind=True, base=BaseTask)
def execute_workflow_async(self, execution_id: str, execution_plan: List[List[WorkflowStep]], input_data: Dict[str, Any]):
    """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®éåŒæœŸå®Ÿè¡Œ"""
    
    try:
        execution = workflow_engine.executions[execution_id]
        step_results = {'input': input_data}
        
        # ãƒ¬ãƒ™ãƒ«ã”ã¨ã®å®Ÿè¡Œ
        for level_index, level_steps in enumerate(execution_plan):
            
            # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒã‚§ãƒƒã‚¯
            if execution['status'] == WorkflowStatus.CANCELLED:
                raise Exception("Workflow cancelled by user")
            
            print(f"Executing level {level_index + 1} with {len(level_steps)} steps")
            
            # ãƒ¬ãƒ™ãƒ«å†…ã®ä¸¦åˆ—å®Ÿè¡Œ
            if len(level_steps) == 1:
                # å˜ä¸€ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè¡Œ
                step = level_steps[0]
                result = execute_workflow_step.delay(
                    step.task_name, 
                    step.parameters, 
                    step_results
                ).get(timeout=step.timeout)
                
                step_results[step.name] = result
                execution['steps'][step.name] = {
                    'status': 'success',
                    'result': result,
                    'execution_time': time.time(),
                    'level': level_index
                }
                
            else:
                # ä¸¦åˆ—ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè¡Œ
                parallel_tasks = []
                
                for step in level_steps:
                    task_signature = signature(
                        'execute_workflow_step',
                        args=[step.task_name, step.parameters, step_results],
                        queue=step.queue,
                        retry=step.retry_count
                    )
                    parallel_tasks.append(task_signature)
                
                # ä¸¦åˆ—å®Ÿè¡Œ
                job = group(parallel_tasks)
                result_group = job.apply_async()
                
                # çµæœã®åé›†
                parallel_results = result_group.get(timeout=max(step.timeout for step in level_steps))
                
                for step, result in zip(level_steps, parallel_results):
                    step_results[step.name] = result
                    execution['steps'][step.name] = {
                        'status': 'success',
                        'result': result,
                        'execution_time': time.time(),
                        'level': level_index
                    }
        
        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Œäº†
        execution['status'] = WorkflowStatus.SUCCESS
        execution['result'] = step_results
        execution['end_time'] = time.time()
        
        print(f"Workflow {execution_id} completed successfully")
        return step_results
        
    except Exception as exc:
        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å¤±æ•—
        execution['status'] = WorkflowStatus.FAILURE
        execution['error'] = str(exc)
        execution['end_time'] = time.time()
        
        logger.error(f"Workflow {execution_id} failed: {exc}")
        raise exc

@app.task(bind=True, base=BaseTask)
def execute_workflow_step(self, task_name: str, parameters: Dict[str, Any], context: Dict[str, Any]):
    """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè¡Œ"""
    
    try:
        # ã‚¿ã‚¹ã‚¯ã®å‹•çš„å®Ÿè¡Œ
        task = app.tasks.get(task_name)
        if not task:
            raise ValueError(f"Task {task_name} not found")
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«çµåˆ
        merged_params = {**context, **parameters}
        
        # ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
        print(f"Executing step: {task_name}")
        result = task(**merged_params)
        print(f"Step {task_name} completed")
        
        return result
        
    except Exception as exc:
        logger.error(f"Workflow step {task_name} failed: {exc}")
        raise exc
```

## å®Ÿç”¨çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®šç¾©

### ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

```python
def register_data_processing_workflow():
    """ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ç™»éŒ²"""
    
    steps = [
        WorkflowStep(
            name="validate_input",
            task_name="validate_data",
            parameters={"validation_rules": "strict"},
            queue="priority"
        ),
        WorkflowStep(
            name="clean_data",
            task_name="clean_dataset",
            parameters={"cleaning_options": {"remove_nulls": True}},
            depends_on=["validate_input"],
            queue="cpu_intensive"
        ),
        WorkflowStep(
            name="transform_data",
            task_name="transform_dataset", 
            parameters={"transformation_rules": "standard"},
            depends_on=["clean_data"],
            queue="cpu_intensive"
        ),
        WorkflowStep(
            name="analyze_data",
            task_name="analyze_dataset",
            parameters={"analysis_type": "comprehensive"},
            depends_on=["transform_data"],
            queue="cpu_intensive"
        ),
        WorkflowStep(
            name="generate_report",
            task_name="generate_analysis_report",
            parameters={"format": "pdf"},
            depends_on=["analyze_data"]
        ),
        WorkflowStep(
            name="send_notification",
            task_name="send_completion_notification",
            parameters={"notification_type": "email"},
            depends_on=["generate_report"]
        )
    ]
    
    workflow_engine.register_workflow("data_processing", steps)
```

### æ¡ä»¶ä»˜ããƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

```python
def register_conditional_workflow():
    """æ¡ä»¶ä»˜ããƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ç™»éŒ²"""
    
    steps = [
        WorkflowStep(
            name="check_prerequisites",
            task_name="check_data_prerequisites",
            parameters={}
        ),
        WorkflowStep(
            name="quick_analysis",
            task_name="quick_data_analysis", 
            parameters={"mode": "fast"},
            depends_on=["check_prerequisites"]
        ),
        WorkflowStep(
            name="detailed_analysis",
            task_name="detailed_data_analysis",
            parameters={"mode": "comprehensive"},
            depends_on=["check_prerequisites"]
        ),
        WorkflowStep(
            name="conditional_processing",
            task_name="conditional_data_processing",
            parameters={
                "conditions": {
                    "data_size_large": {
                        "type": "value_greater_than",
                        "params": {"field": "data_size", "threshold": 1000000},
                        "task": "detailed_analysis"
                    },
                    "data_size_small": {
                        "type": "value_greater_than",
                        "params": {"field": "data_size", "threshold": 10000},
                        "task": "quick_analysis"
                    }
                }
            },
            depends_on=["check_prerequisites"]
        )
    ]
    
    workflow_engine.register_workflow("conditional_processing", steps)
```

### è¤‡é›‘ãªä¸¦åˆ—ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

```python
def register_parallel_processing_workflow():
    """ä¸¦åˆ—å‡¦ç†ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ç™»éŒ²"""
    
    steps = [
        # åˆæœŸåŒ–ãƒ•ã‚§ãƒ¼ã‚º
        WorkflowStep(
            name="initialize",
            task_name="initialize_processing",
            parameters={"config": "production"}
        ),
        
        # ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿æº–å‚™ãƒ•ã‚§ãƒ¼ã‚º
        WorkflowStep(
            name="prepare_user_data",
            task_name="prepare_user_data",
            parameters={"data_type": "users"},
            depends_on=["initialize"],
            queue="io_intensive"
        ),
        WorkflowStep(
            name="prepare_product_data",
            task_name="prepare_product_data", 
            parameters={"data_type": "products"},
            depends_on=["initialize"],
            queue="io_intensive"
        ),
        WorkflowStep(
            name="prepare_analytics_data",
            task_name="prepare_analytics_data",
            parameters={"data_type": "analytics"},
            depends_on=["initialize"],
            queue="io_intensive"
        ),
        
        # ä¸¦åˆ—åˆ†æãƒ•ã‚§ãƒ¼ã‚º
        WorkflowStep(
            name="analyze_user_behavior",
            task_name="analyze_user_behavior",
            parameters={"analysis_type": "behavioral"},
            depends_on=["prepare_user_data", "prepare_analytics_data"],
            queue="cpu_intensive"
        ),
        WorkflowStep(
            name="analyze_product_performance",
            task_name="analyze_product_performance",
            parameters={"analysis_type": "performance"},
            depends_on=["prepare_product_data", "prepare_analytics_data"],
            queue="cpu_intensive"
        ),
        
        # çµ±åˆãƒ•ã‚§ãƒ¼ã‚º
        WorkflowStep(
            name="integrate_results",
            task_name="integrate_analysis_results",
            parameters={"integration_method": "comprehensive"},
            depends_on=["analyze_user_behavior", "analyze_product_performance"],
            timeout=600
        ),
        
        # æœ€çµ‚å‡ºåŠ›ãƒ•ã‚§ãƒ¼ã‚º
        WorkflowStep(
            name="generate_dashboard",
            task_name="generate_dashboard",
            parameters={"dashboard_type": "executive"},
            depends_on=["integrate_results"]
        ),
        WorkflowStep(
            name="send_alerts",
            task_name="send_threshold_alerts",
            parameters={"alert_level": "high"},
            depends_on=["integrate_results"]
        )
    ]
    
    workflow_engine.register_workflow("parallel_processing", steps)
```

## ä½¿ç”¨ä¾‹ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

### ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œä¾‹

```python
def execute_data_processing_example():
    """ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å®Ÿè¡Œä¾‹"""
    
    input_data = {
        "dataset_id": 123,
        "processing_options": {
            "quality_threshold": 0.95,
            "output_format": "json"
        },
        "notification_recipients": ["admin@example.com"]
    }
    
    execution_id = workflow_engine.execute_workflow("data_processing", input_data)
    
    # å®Ÿè¡ŒçŠ¶æ³ã®ç›£è¦–
    import time
    while True:
        status = workflow_engine.get_workflow_status(execution_id)
        print(f"Status: {status['status']}")
        
        if status['status'] in [WorkflowStatus.SUCCESS, WorkflowStatus.FAILURE, WorkflowStatus.CANCELLED]:
            break
            
        if status['status'] == WorkflowStatus.RUNNING:
            progress = status.get('progress', {})
            print(f"Progress: {progress.get('percentage', 0)}% ({progress.get('completed_steps', 0)}/{progress.get('total_steps', 0)})")
        
        time.sleep(5)
    
    return execution_id

# åˆæœŸåŒ–æ™‚ã«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ç™»éŒ²
register_data_processing_workflow()
register_conditional_workflow()
register_parallel_processing_workflow()
```

### ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç®¡ç†ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£

```python
class WorkflowManager:
    """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç®¡ç†ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""
    
    def __init__(self, engine: WorkflowEngine):
        self.engine = engine
    
    def list_workflows(self) -> List[str]:
        """ç™»éŒ²æ¸ˆã¿ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä¸€è¦§"""
        return list(self.engine.workflows.keys())
    
    def get_workflow_info(self, workflow_id: str) -> Dict[str, Any]:
        """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æƒ…å ±ã®å–å¾—"""
        if workflow_id not in self.engine.workflows:
            raise ValueError(f"Workflow {workflow_id} not found")
        
        steps = self.engine.workflows[workflow_id]
        return {
            'workflow_id': workflow_id,
            'step_count': len(steps),
            'steps': [
                {
                    'name': step.name,
                    'task_name': step.task_name,
                    'depends_on': step.depends_on,
                    'queue': step.queue,
                    'timeout': step.timeout
                }
                for step in steps
            ]
        }
    
    def cancel_all_executions(self):
        """å…¨å®Ÿè¡Œã®ã‚­ãƒ£ãƒ³ã‚»ãƒ«"""
        for execution_id in self.engine.executions:
            if self.engine.executions[execution_id]['status'] == WorkflowStatus.RUNNING:
                self.engine.cancel_workflow(execution_id)
    
    def cleanup_completed_executions(self, max_age_hours: int = 24):
        """å®Œäº†ã—ãŸå®Ÿè¡Œã®æ¸…ç†"""
        cutoff_time = time.time() - (max_age_hours * 3600)
        
        to_remove = []
        for execution_id, execution in self.engine.executions.items():
            if (execution.get('end_time', time.time()) < cutoff_time and 
                execution['status'] in [WorkflowStatus.SUCCESS, WorkflowStatus.FAILURE]):
                to_remove.append(execution_id)
        
        for execution_id in to_remove:
            del self.engine.executions[execution_id]
        
        print(f"Cleaned up {len(to_remove)} completed executions")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ç®¡ç†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
workflow_manager = WorkflowManager(workflow_engine)
```