# SQLAlchemy 2.0 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š

éåŒæœŸã‚¨ãƒ³ã‚¸ãƒ³ã€æ¥ç¶šãƒ—ãƒ¼ãƒ«ã€ã‚«ã‚¹ã‚¿ãƒ å‹å®šç¾©ã®ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³å‘ã‘è¨­å®šã€‚

## ğŸ”§ åŸºæœ¬è¨­å®š

```python
# database/config.py
from sqlalchemy.ext.asyncio import (
    create_async_engine, 
    AsyncSession, 
    async_sessionmaker,
    AsyncEngine
)
from sqlalchemy.orm import DeclarativeBase, MappedAsDataclass
from sqlalchemy.pool import NullPool, QueuePool
from sqlalchemy import event, text
from contextlib import asynccontextmanager

class Base(MappedAsDataclass, DeclarativeBase):
    """SQLAlchemy 2.0 ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹ - ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹çµ±åˆ"""
    pass

class DatabaseManager:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç®¡ç†"""
    
    async def initialize(self):
        """ã‚¨ãƒ³ã‚¸ãƒ³ã¨ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–"""
        
        # ç’°å¢ƒåˆ¥ã‚¨ãƒ³ã‚¸ãƒ³è¨­å®š
        if settings.ENVIRONMENT == "test":
            # ãƒ†ã‚¹ãƒˆç’°å¢ƒ: è»½é‡è¨­å®š
            self.engine = create_async_engine(
                settings.DATABASE_URL,
                poolclass=NullPool,  # æ¥ç¶šãƒ—ãƒ¼ãƒ«ç„¡åŠ¹
                isolation_level="AUTOCOMMIT",
            )
        else:
            # ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒ: æœ€é©åŒ–è¨­å®š
            self.engine = create_async_engine(
                settings.DATABASE_URL,
                poolclass=QueuePool,
                pool_size=20,           # åŸºæœ¬æ¥ç¶šæ•°
                max_overflow=10,        # è¿½åŠ æ¥ç¶šæ•°
                pool_timeout=30,        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                pool_pre_ping=True,     # å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
                pool_recycle=3600,      # 1æ™‚é–“ã§ãƒªã‚µã‚¤ã‚¯ãƒ«
                connect_args={
                    "command_timeout": 60,
                    "server_settings": {
                        "application_name": settings.APP_NAME,
                        "jit": "off"
                    }
                }
            )
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼
        self.session_factory = async_sessionmaker(
            bind=self.engine,
            class_=AsyncSession,
            expire_on_commit=False,  # ã‚³ãƒŸãƒƒãƒˆå¾Œã‚‚ä½¿ç”¨å¯èƒ½
            autoflush=True,
            autocommit=False,
        )
        
        # æ¥ç¶šãƒ†ã‚¹ãƒˆ
        async with self.engine.begin() as conn:
            await conn.execute(text("SELECT 1"))
    
    @asynccontextmanager
    async def get_session(self) -> AsyncGenerator[AsyncSession, None]:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³å–å¾—ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
        async with self.session_factory() as session:
            try:
                yield session
                await session.commit()
            except Exception:
                await session.rollback()
                raise
            finally:
                await session.close()
    
    async def close(self):
        """æ¥ç¶šã‚¯ãƒ­ãƒ¼ã‚º"""
        if self.engine:
            await self.engine.dispose()

# ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
db_manager = DatabaseManager()
```

## ğŸ¯ ã‚«ã‚¹ã‚¿ãƒ å‹å®šç¾©

```python
# database/types.py
from sqlalchemy.types import TypeDecorator, String, JSON
from sqlalchemy.dialects.postgresql import UUID, ARRAY, JSONB
import json
import uuid

class UUIDType(TypeDecorator):
    """UUIDå‹ï¼ˆDBéä¾å­˜ï¼‰"""
    impl = String(36)
    cache_ok = True
    
    def process_bind_param(self, value, dialect):
        if value is None:
            return value
        if dialect.name == 'postgresql':
            return str(value)
        return str(value).replace('-', '')
    
    def process_result_value(self, value, dialect):
        if value is None:
            return value
        return uuid.UUID(value)

class EncryptedType(TypeDecorator):
    """æš—å·åŒ–ã‚«ãƒ©ãƒ å‹"""
    impl = String
    cache_ok = True
    
    def process_bind_param(self, value, dialect):
        if value is not None:
            return encrypt(value)  # æš—å·åŒ–é–¢æ•°
        return value
    
    def process_result_value(self, value, dialect):
        if value is not None:
            return decrypt(value)  # å¾©å·åŒ–é–¢æ•°
        return value

class JSONBType(TypeDecorator):
    """JSONBå‹ï¼ˆPostgreSQLæœ€é©åŒ–ï¼‰"""
    impl = JSON
    cache_ok = True
    
    def load_dialect_impl(self, dialect):
        if dialect.name == 'postgresql':
            return dialect.type_descriptor(JSONB)
        else:
            return dialect.type_descriptor(JSON)

# ä½¿ç”¨ä¾‹
class User(Base):
    __tablename__ = "users"
    
    id: Mapped[uuid.UUID] = mapped_column(UUIDType, primary_key=True)
    encrypted_data: Mapped[str] = mapped_column(EncryptedType)
    settings: Mapped[dict] = mapped_column(JSONBType)
    tags: Mapped[List[str]] = mapped_column(ARRAY(String))
```

## âš™ï¸ ãƒŸãƒƒã‚¯ã‚¹ã‚¤ãƒ³

```python
# database/mixins.py
from sqlalchemy import DateTime, func
from datetime import datetime

class TimestampMixin:
    """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒŸãƒƒã‚¯ã‚¹ã‚¤ãƒ³"""
    created_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True),
        server_default=func.now(),
        nullable=False
    )
    updated_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True),
        server_default=func.now(),
        onupdate=func.now(),
        nullable=False
    )

class SoftDeleteMixin:
    """è«–ç†å‰Šé™¤ãƒŸãƒƒã‚¯ã‚¹ã‚¤ãƒ³"""
    is_deleted: Mapped[bool] = mapped_column(
        default=False,
        index=True  # æ¤œç´¢æ€§èƒ½å‘ä¸Š
    )
    deleted_at: Mapped[Optional[datetime]] = mapped_column(
        DateTime(timezone=True),
        nullable=True
    )

class VersionMixin:
    """æ¥½è¦³çš„ãƒ­ãƒƒã‚¯ç”¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†"""
    version: Mapped[int] = mapped_column(
        default=1,
        nullable=False
    )
    
    __mapper_args__ = {
        "version_id_col": version,
        "version_id_generator": lambda v: (v or 0) + 1
    }
```

## ğŸ”Œ æ¥ç¶šãƒ—ãƒ¼ãƒ«ç›£è¦–

```python
# database/monitoring.py
class PoolMonitor:
    """æ¥ç¶šãƒ—ãƒ¼ãƒ«ç›£è¦–"""
    
    @staticmethod
    def setup_listeners(engine):
        """ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼è¨­å®š"""
        
        @event.listens_for(engine, "connect")
        def receive_connect(dbapi_conn, connection_record):
            connection_record.info['connect_time'] = time.time()
        
        @event.listens_for(engine, "checkout")
        def receive_checkout(dbapi_conn, connection_record, connection_proxy):
            duration = time.time() - connection_record.info.get('connect_time', 0)
            if duration > 300:  # 5åˆ†ä»¥ä¸Š
                logger.warning(f"Long-lived connection: {duration}s")
        
        @event.listens_for(engine, "checkin")
        def receive_checkin(dbapi_conn, connection_record):
            # æ¥ç¶šã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
            if connection_record.info.get('invalidated'):
                logger.info("Invalidated connection returned to pool")
    
    @staticmethod
    def get_pool_status(engine):
        """ãƒ—ãƒ¼ãƒ«çŠ¶æ…‹å–å¾—"""
        pool = engine.pool
        return {
            "size": pool.size(),
            "checked_out": pool.checked_out(),
            "overflow": pool.overflow(),
            "total": pool.size() + pool.overflow()
        }
```

## ğŸ’¡ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### æ¥ç¶šãƒ—ãƒ¼ãƒ«è¨­å®š
- **pool_size**: CPUæ•° Ã— 2-4
- **max_overflow**: pool_sizeã®50%
- **pool_pre_ping**: æœ¬ç•ªç’°å¢ƒã§ã¯å¿…é ˆ
- **pool_recycle**: ã‚¯ãƒ©ã‚¦ãƒ‰ç’°å¢ƒã§ã¯å¿…é ˆ

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
- **expire_on_commit=False**: ä¸è¦ãªãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥å›é¿
- **JSONBä½¿ç”¨**: PostgreSQLã§ã®JSONæœ€é©åŒ–
- **ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹**: æ¤œç´¢ãƒ»çµåˆã‚«ãƒ©ãƒ ã«å¿…é ˆ
- **ãƒãƒƒãƒå‡¦ç†**: bulk_insert_mappingsæ´»ç”¨